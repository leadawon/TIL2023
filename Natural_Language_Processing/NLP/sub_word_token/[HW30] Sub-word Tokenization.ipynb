{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium","widgets":{"application/vnd.jupyter.widget-state+json":{"7c49a7683e6f4f559c701d7640924afa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_55c6f4db2cdc4a59a0d546ee93c16d9b","IPY_MODEL_ff6ed305311f4ea096ed2abe2d421a34","IPY_MODEL_fb178ef8a31f48209033d25deca06ea8"],"layout":"IPY_MODEL_57bc324389ca446cb222d50b1bb4f2b9"}},"55c6f4db2cdc4a59a0d546ee93c16d9b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd648c27d8a9497d9f7c96c3ff70297b","placeholder":"‚Äã","style":"IPY_MODEL_a74576bb5f3b43a4b038e17643bf3b3c","value":"Downloading: 100%"}},"ff6ed305311f4ea096ed2abe2d421a34":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ad2fd0274dd4d0982be8c1aebde5a3a","max":213450,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3eb1c0369ebd421484dcc22d4d8e9a0d","value":213450}},"fb178ef8a31f48209033d25deca06ea8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e40ee12c9299448086695fb2c8d182ee","placeholder":"‚Äã","style":"IPY_MODEL_53a520ec6f8c40b097a2e6dbff9dca77","value":" 213k/213k [00:00&lt;00:00, 278kB/s]"}},"57bc324389ca446cb222d50b1bb4f2b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd648c27d8a9497d9f7c96c3ff70297b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a74576bb5f3b43a4b038e17643bf3b3c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ad2fd0274dd4d0982be8c1aebde5a3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3eb1c0369ebd421484dcc22d4d8e9a0d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e40ee12c9299448086695fb2c8d182ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53a520ec6f8c40b097a2e6dbff9dca77":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b9244bdfc67947cca4a5bde93eedc672":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bfe9d98cb03b4feea392a182a0e5f79c","IPY_MODEL_556d49f369e84355909b800ca3f2d60b","IPY_MODEL_92d3b6653ef844f28c75da2b436913df"],"layout":"IPY_MODEL_6ed85b6918384d79b4aa944e4eb0ba1d"}},"bfe9d98cb03b4feea392a182a0e5f79c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee003584886443beabcda3e3dd12fba2","placeholder":"‚Äã","style":"IPY_MODEL_a38f6df0c3824184a7b133d9b3e90c06","value":"Downloading: 100%"}},"556d49f369e84355909b800ca3f2d60b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4080c3d775e34c8d92b7d0e3fcc44e2b","max":29,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d4413acd32c148b384cb2b5bb4a30cf4","value":29}},"92d3b6653ef844f28c75da2b436913df":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ae5d00f82aa4179b1b539205adb4045","placeholder":"‚Äã","style":"IPY_MODEL_02b56caae6ff403c816f32b854753bca","value":" 29.0/29.0 [00:00&lt;00:00, 2.12kB/s]"}},"6ed85b6918384d79b4aa944e4eb0ba1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee003584886443beabcda3e3dd12fba2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a38f6df0c3824184a7b133d9b3e90c06":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4080c3d775e34c8d92b7d0e3fcc44e2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4413acd32c148b384cb2b5bb4a30cf4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1ae5d00f82aa4179b1b539205adb4045":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02b56caae6ff403c816f32b854753bca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"06f6fc03e95b4257857c7b5e84b30ef1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_db28a66bb471415ba7f2b4a57f3a506d","IPY_MODEL_82b75640d8a149a9aaa764b1b6424d31","IPY_MODEL_f12953a685e647808a209960fc81d2ac"],"layout":"IPY_MODEL_ce49144862f44c6f9ced7c3994fa66ad"}},"db28a66bb471415ba7f2b4a57f3a506d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5d4171ef3f54360adaea48a9f2c0c45","placeholder":"‚Äã","style":"IPY_MODEL_7859eb3f3c4946aab87b3fc436d406e9","value":"Downloading: 100%"}},"82b75640d8a149a9aaa764b1b6424d31":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_01501971bb4a4514b3e5017828eb031c","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bfa30fa76d5d4b45bf2a23e65c81364c","value":570}},"f12953a685e647808a209960fc81d2ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a76ab16665de47f4b0648dd0345f29ad","placeholder":"‚Äã","style":"IPY_MODEL_d4ae85dbb3c042bdb7a033359c921ae3","value":" 570/570 [00:00&lt;00:00, 41.4kB/s]"}},"ce49144862f44c6f9ced7c3994fa66ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5d4171ef3f54360adaea48a9f2c0c45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7859eb3f3c4946aab87b3fc436d406e9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"01501971bb4a4514b3e5017828eb031c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfa30fa76d5d4b45bf2a23e65c81364c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a76ab16665de47f4b0648dd0345f29ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4ae85dbb3c042bdb7a033359c921ae3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"uOnz8OxdbN3y"},"source":["# Tokenization\n"]},{"cell_type":"markdown","metadata":{"id":"TZb_LOZr7XF_"},"source":["### Data Preparation\n","\n","\n","*   Îç∞Ïù¥ÌÑ∞(train, dev, test)Í∞Ä Ï†ÄÏû•Îêú ÎìúÎùºÏù¥Î∏åÎ•º ÎßàÏö¥Ìä∏ÌïòÏó¨ Îç∞Ïù¥ÌÑ∞Î•º Î∂àÎü¨Ïò¨ Ïàò ÏûàÎäî Í≤ΩÎ°úÎ•º Ï§ÄÎπÑÌï©ÎãàÎã§.\n","\n"]},{"cell_type":"code","metadata":{"id":"wkCrAHWVmUck","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671338515689,"user_tz":-540,"elapsed":2856,"user":{"displayName":"Ïù¥Îã§Ïõê","userId":"11923633709773630529"}},"outputId":"3544f09b-f78a-457a-a888-065a97e3402c"},"source":["import os, sys\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","\n","# ÌòÑÏû¨ ÌååÏùº Í≤ΩÎ°úÏóê ÎßûÍ≤å ÏÑ§Ï†ïÌï¥Ï£ºÏÑ∏Ïöî.\n","os.chdir('/content/drive/MyDrive/WARNING_PRIVATE_FOLDER/TIL2023/Natural_Language_Processing/NLP/sub_word_token/')"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NSVysy6N4QhW","executionInfo":{"status":"ok","timestamp":1671338518865,"user_tz":-540,"elapsed":3184,"user":{"displayName":"Ïù¥Îã§Ïõê","userId":"11923633709773630529"}},"outputId":"e5e2490f-44d1-4ac4-c796-64d206fbc57f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"e9yYsxMfyZsi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671338518865,"user_tz":-540,"elapsed":15,"user":{"displayName":"Ïù¥Îã§Ïõê","userId":"11923633709773630529"}},"outputId":"2dfbf249-c02e-4c5e-c5d1-662608a1f416"},"source":["# ÌòÑÏû¨ Í≤ΩÎ°ú ÌôïÏù∏\n","!pwd"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/WARNING_PRIVATE_FOLDER/TIL2023/Natural_Language_Processing/NLP/sub_word_token\n"]}]},{"cell_type":"code","metadata":{"id":"ceOZZ8UOU4or","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671338531744,"user_tz":-540,"elapsed":11039,"user":{"displayName":"Ïù¥Îã§Ïõê","userId":"11923633709773630529"}},"outputId":"f7adf7e7-f399-4094-80eb-b541ba2330e7"},"source":["# ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÑ§Ïπò\n","!pip install transformers"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.8 MB 29.6 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.6 MB 82.6 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182 kB 101.5 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"]}]},{"cell_type":"markdown","metadata":{"id":"NSNON1TAbj2i"},"source":["### Introduction\n","\n","\n","* Î≥∏ Í≥ºÏ†úÏùò Î™©Ï†ÅÏùÄ Subword tokenizationÏùò ÌïÑÏöîÏÑ±ÏùÑ ÏßÅÏ†ë ÎäêÍª¥Î≥¥Îäî Í≤ÉÏûÖÎãàÎã§.\n","* Subword tokenization Í∏∞Î∞ò language modelÏùÑ Íµ¨ÌòÑÌïòÎ©¥ÏÑú Ïù¥Ï†Ñ Í≥ºÏ†úÏùò Word-level language modelÍ≥º ÎπÑÍµêÌï¥Î≥¥Îäî ÏãúÍ∞ÑÏùÑ Í∞ñÍ≤†ÏäµÎãàÎã§. Ï∂îÍ∞ÄÏ†ÅÏúºÎ°ú RNNÏùÑ LSTMÏúºÎ°ú Î≥ÄÍ≤ΩÌñàÏùÑ ÎïåÏùò ÏÑ±Îä• Ï∞®Ïù¥Ïóê ÎåÄÌï¥ ÏÇ¥Ìé¥Î≥¥Í≤†ÏäµÎãàÎã§.\n","*   Subword-level language modelÏùÑ Íµ¨ÌòÑÌïòÍ≥†, Ï£ºÏñ¥ÏßÑ Îç∞Ïù¥ÌÑ∞Î•º Í∞ÄÍ≥µÌïòÏó¨ Î™®Îç∏ÏùÑ ÌïôÏäµÌïú ÌõÑ ÌïôÏäµÎêú Ïñ∏Ïñ¥ Î™®Îç∏ÏùÑ Ïù¥Ïö©Ìï¥ Î¨∏Ïû•ÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.\n"]},{"cell_type":"markdown","metadata":{"id":"_xzSMF9RJyzg"},"source":["\n","```\n","üí° SubwordÎäî Î¨¥ÏóáÏù∏Í∞ÄÏöî?\n","\n","SubwordÎäî ÌïòÎÇòÏùò Îã®Ïñ¥Î•º Ïó¨Îü¨Í∞úÏùò Îã®ÏúÑÎ°ú Î∂ÑÎ¶¨ÌñàÏùÑ Îïå ÌïòÎÇòÏùò Îã®ÏúÑÎ•º ÎÇòÌÉÄÎÉÖÎãàÎã§. \"subword\"Î•º subword Îã®ÏúÑÎ°ú ÎÇòÌÉÄÎÇ∏ ÌïòÎÇòÏùò ÏòàÏãúÎäî Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§.\n","\n","\"sub\" + \"word\"\n","\n","subÎùºÎäî Ï†ëÎëêÏÇ¨ÏôÄ wordÎùºÍ≥† ÌïòÎäî Ïñ¥Í∑ºÏúºÎ°ú ÎÇòÎàÑÏñ¥ \"subword\"ÎùºÍ≥† ÌïòÎäî wordÎ•º 2Í∞úÏùò subwordÎ°ú ÎÇòÌÉÄÎÉàÏäµÎãàÎã§.\n","\n","Ïù¥Ïô∏ÏóêÎèÑ Îã§ÏñëÌïú ÌòïÌÉúÏùò subwordÎ°ú ÎÇòÌÉÄÎÇº Ïàò ÏûàÏäµÎãàÎã§. (e.g., \"su\" + \"bword\", \"s\" + \"ubword\", \"subwor\" + \"d\")\n","```"]},{"cell_type":"markdown","metadata":{"id":"zrgY-yRfVJON"},"source":["```\n","üí° tokenizationÏùÄ Î¨¥ÏóáÏù∏Í∞ÄÏöî?\n","\n","tokenizationÏùÄ Ï£ºÏñ¥ÏßÑ ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞Î•º ÏûêÏó∞Ïñ¥Ï≤òÎ¶¨ Î™®Îç∏Ïù¥ Ïù∏ÏãùÌï† Ïàò ÏûàÎäî Îã®ÏúÑÎ°ú Î≥ÄÌôòÌï¥Ï£ºÎäî Î∞©Î≤ïÏûÖÎãàÎã§. \n","\n","üí° word tokenizationÏùÄÏöî?\n","\n","word tokenizationÏùò Í≤ΩÏö∞ \"Îã®Ïñ¥\"Í∞Ä ÏûêÏó∞Ïñ¥Ï≤òÎ¶¨ Î™®Îç∏Ïù¥ Ïù∏ÏãùÌïòÎäî Îã®ÏúÑÍ∞Ä Îê©ÎãàÎã§.\n","\"I have a meal\"Ïù¥ÎùºÍ≥† ÌïòÎäî Î¨∏Ïû•ÏùÑ Í∞ÄÏßÄÍ≥† word tokenizationÏùÑ ÌïòÎ©¥ Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§. \n","\n","- ['I', 'have', 'a', 'meal']\n","\n","ÏòÅÏñ¥Ïùò Í≤ΩÏö∞ ÎåÄÎ∂ÄÎ∂Ñ spaceÎ•º Í∏∞Ï§ÄÏúºÎ°ú Îã®Ïñ¥Í∞Ä Ï†ïÏùòÎêòÍ∏∞ ÎïåÎ¨∏Ïóê .split()ÏùÑ Ïù¥Ïö©Ìï¥ ÏâΩÍ≤å word tokenizationÏùÑ Íµ¨ÌòÑÌï† Ïàò ÏûàÏäµÎãàÎã§.\n","ÏòÅÏñ¥ÏóêÏÑú word tokenizationÏùÄ space tokenizationÏù¥ÎùºÍ≥†ÎèÑ Ìï† Ïàò ÏûàÍ≥†, \n","subword tokenization Ïù¥Ï†ÑÏóê ÏàòÌñâÎêòÎäî pre-tokenization Î∞©Î≤ïÏúºÎ°úÎèÑ ÎßéÏù¥ ÏÇ¨Ïö©Îê©ÎãàÎã§.\n","\n","\"ÎÇòÎäî Î∞•ÏùÑ Î®πÎäîÎã§\"ÎùºÎäî Î¨∏Ïû•ÏùÑ word tokenizationÌïòÎ©¥ Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§.\n","- ['ÎÇò', 'Îäî', 'Î∞•', 'ÏùÑ', 'Î®πÎäîÎã§']\n","\n","ÌïúÍµ≠Ïñ¥ÏóêÏÑú \"Îã®Ïñ¥\"Îäî Í≥µÎ∞±(space)ÏùÑ Í∏∞Ï§ÄÏúºÎ°ú Ï†ïÏùòÎêòÏßÄ ÏïäÏäµÎãàÎã§. Ïù¥Îäî ÌïúÍµ≠Ïñ¥Í∞Ä Í∞ñÍ≥† ÏûàÎäî \"ÍµêÏ∞©Ïñ¥\"Î°úÏÑúÏùò ÌäπÏßï ÎïåÎ¨∏ÏûÖÎãàÎã§. \n","Ï≤¥Ïñ∏ Îí§Ïóê Ï°∞ÏÇ¨Í∞Ä Î∂ôÎäî Í≤ÉÏù¥ ÎåÄÌëúÏ†ÅÏù∏ ÌäπÏßïÏù¥Î©∞ ÏùòÎØ∏ Îã®ÏúÑÍ∞Ä Íµ¨Î∂ÑÎêòÍ≥† ÏûêÎ¶ΩÏÑ±Ïù¥ ÏûàÍ∏∞ ÎïåÎ¨∏Ïóê Ï°∞ÏÇ¨Îäî \"Îã®Ïñ¥\"ÏûÖÎãàÎã§.\n","\n","ÌïúÍµ≠Ïñ¥ÏóêÏÑúÎäî pre-tokenization Î∞©Î≤ïÏúºÎ°ú space tokenizationÏùÑ ÏÇ¨Ïö©ÌïòÏßÄ ÏïäÍ≥† ÌòïÌÉúÏÜå Î∂ÑÏÑùÍ∏∞Î•º ÌôúÏö©ÌïòÍ≥† ÏûàÏäµÎãàÎã§.\n","```\n","\n","(Ï∞∏Í≥†1: [Íµ≠Î¶Ω Íµ≠Ïñ¥Ïõê: \"Ï°∞ÏÇ¨Îäî Îã®Ïñ¥Ïù¥Îã§\"](https://www.korean.go.kr/front/onlineQna/onlineQnaView.do?mn_id=216&qna_seq=26915#:~:text='%EC%A1%B0%EC%82%AC'%EB%8A%94%20%EC%99%84%EC%A0%84%ED%95%9C%20%EC%9E%90%EB%A6%BD%EC%84%B1%EC%9D%80,%ED%95%98%EC%97%AC%20%EB%8B%A8%EC%96%B4%EB%A1%9C%20%EC%B2%98%EB%A6%AC%ED%95%A9%EB%8B%88%EB%8B%A4.) )\n","\n","(Ï∞∏Í≥†2: [Huggingface: Pre-tokenization](https://huggingface.co/docs/tokenizers/python/latest/pipeline.html#pre-tokenization))\n","\n","(Ï∞∏Í≥†3: [Konlpy: ÌòïÌÉúÏÜå Î∂ÑÏÑùÍ∏∞](https://konlpy-ko.readthedocs.io/ko/v0.4.3/morph/))"]},{"cell_type":"markdown","metadata":{"id":"BVQgjQJdAbWh"},"source":["```\n","üí° Í∑∏Îüº Subword tokenizationÏùÄ Î¨¥ÏóáÏù∏Í∞ÄÏöî?\n","\n","Subword tokenizaitonÏùÄ Îßê Í∑∏ÎåÄÎ°ú subword Îã®ÏúÑÎ°ú tokenizationÏùÑ ÌïúÎã§Îäî ÎúªÏûÖÎãàÎã§.\n","Î∞©Í∏à Ï†Ñ word tokenizationÏùÑ ÏàòÌñâÌñàÎçò Î¨∏Ïû•ÏùÑ Ïù¥Ïö©Ìï¥ subword tokenizationÏùÑ ÏàòÌñâÌïú ÏòàÏãúÎ•º Î≥¥Í≤†ÏäµÎãàÎã§.\n","\n","Subword tokenizationÏùÑ Ï†ÅÏö©ÌñàÏùÑ ÎïåÎäî Îã§ÏùåÍ≥º Í∞ôÏù¥ tokenizationÏù¥ Îê† Ïàò ÏûàÏäµÎãàÎã§.\n","\n","Example 1\n","\n","\"I have a meal\" -> ['I', 'hav', 'e', 'a', 'me', 'al']\n","\"ÎÇòÎäî Î∞•ÏùÑ Î®πÎäîÎã§\" -> ['ÎÇò', 'Îäî', 'Î∞•', 'ÏùÑ', 'Î®πÎäî', 'Îã§']\n","\n","word Îã®ÏúÑÍ∞Ä ÏïÑÎãàÎùº Í∑∏Î≥¥Îã§ Îçî ÏûòÍ≤å Ï™ºÍ∞† subword Îã®ÏúÑÎ°ú Î¨∏Ïû•ÏùÑ tokenizationÌï©ÎãàÎã§.\n","\n","ÏúÑÏóêÏÑú ÎßêÏîÄÎìúÎ¶∞ Í≤ÉÍ≥º Í∞ôÏù¥ Ïó¨Îü¨Í∞ÄÏßÄ Í≤ΩÏö∞Ïùò ÏàòÍ∞Ä Í∞ÄÎä•Ìï©ÎãàÎã§.\n","\n","Example 2\n","\n","\"I have a meal\" -> ['I', 'ha', 've', 'a', 'mea', 'l']\n","\"ÎÇòÎäî Î∞•ÏùÑ Î®πÎäîÎã§\" -> ['ÎÇò', 'Îäî', 'Î∞•', 'ÏùÑ', 'Î®π', 'ÎäîÎã§']\n","\n","Í∑∏Î†áÏßÄÎßå Í∏∞Î≥∏Ï†ÅÏúºÎ°ú Í≥µÎ∞±ÏùÑ ÎÑòÏñ¥ÏÑ† subwordÎ•º Íµ¨ÏÑ±ÌïòÏßÑ ÏïäÏäµÎãàÎã§.\n","ÏòàÎ•º Îì§Ïñ¥ Îã§ÏùåÍ≥º Í∞ôÏù¥ tokenizaitonÏùÑ ÏàòÌñâÌïòÏßÑ ÏïäÏäµÎãàÎã§.\n","\n","Example 3\n","\n","\"I have a meal\" -> ['Iha', 've', 'am', 'ea', 'l']\n","\"ÎÇòÎäî Î∞•ÏùÑ Î®πÎäîÎã§\" -> ['ÎÇòÎäîÎ∞•', 'ÏùÑÎ®π', 'ÎäîÎã§']\n","```\n","\n","(Ï∞∏Í≥†4: [Huggingface: subword-tokenization](https://huggingface.co/transformers/tokenizer_summary.html#subword-tokenization))"]},{"cell_type":"markdown","metadata":{"id":"SPRNaFhMEK67"},"source":["```\n","üí° Subword tokenizationÏùÄ Ïôú ÌïÑÏöîÌïúÍ∞ÄÏöî?\n","\n","word tokenization ÏΩîÎìúÎ•º Î∂àÎü¨ÏôÄ Í∑∏ ÌïÑÏöîÏÑ±ÏùÑ ÏÉùÍ∞ÅÌï¥ Î¥ÖÏãúÎã§.\n","\n","```"]},{"cell_type":"code","metadata":{"id":"DESsQzhwGVST","executionInfo":{"status":"ok","timestamp":1671338759508,"user_tz":-540,"elapsed":2866,"user":{"displayName":"Ïù¥Îã§Ïõê","userId":"11923633709773630529"}}},"source":["import os\n","from io import open\n","import torch"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"SWd09aUBGWa9","executionInfo":{"status":"ok","timestamp":1671338759508,"user_tz":-540,"elapsed":6,"user":{"displayName":"Ïù¥Îã§Ïõê","userId":"11923633709773630529"}}},"source":["class Dictionary(object):\n","    def __init__(self):\n","        self.word2idx = {'<unk>': 0}\n","        self.idx2word = ['<unk>']\n","\n","    def add_word(self, word):\n","        if word not in self.word2idx:\n","            self.idx2word.append(word)\n","            self.word2idx[word] = len(self.idx2word) - 1\n","        return self.word2idx[word]\n","\n","    def __len__(self):\n","        return len(self.idx2word)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"xQIyGQlfd9Os","executionInfo":{"status":"ok","timestamp":1671338958805,"user_tz":-540,"elapsed":545,"user":{"displayName":"Ïù¥Îã§Ïõê","userId":"11923633709773630529"}}},"source":["class Corpus(object):\n","    def __init__(self, path):\n","        self.dictionary = Dictionary()\n","        \"\"\"Tokenizes a text file.\"\"\"\n","        assert os.path.exists(path)\n","        # Add words to the dictionary\n","        with open(os.path.join(path, 'train.txt'), 'r', encoding=\"utf8\") as f:\n","            for line in f:\n","                words = line.split() + ['<eos>']\n","                for word in words:\n","                    self.dictionary.add_word(word)\n","\n","        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n","        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n","        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n","\n","    def tokenize(self, path):\n","        # Tokenize file content\n","        with open(path, 'r', encoding=\"utf8\") as f:\n","            idss = []\n","            for line in f:\n","                words = line.split() + ['<eos>']\n","                ids = []\n","                for word in words:\n","                    try:\n","                        ids.append(self.dictionary.word2idx[word])\n","                    except:\n","                        print(word)\n","                        ids.append(0)\n","                idss.append(torch.tensor(ids).type(torch.int64))\n","            ids = torch.cat(idss)\n","\n","        return ids"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"a55D0z53eLzB","executionInfo":{"status":"ok","timestamp":1671339300557,"user_tz":-540,"elapsed":471,"user":{"displayName":"Ïù¥Îã§Ïõê","userId":"11923633709773630529"}}},"source":["import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class RNNModel(nn.Module):\n","    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n","\n","    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5):\n","        super(RNNModel, self).__init__()\n","        self.ntoken = ntoken\n","        self.drop = nn.Dropout(dropout)\n","        self.encoder = nn.Embedding(ntoken, ninp)\n","        if rnn_type in ['LSTM', 'GRU']:\n","            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout) #nn.rnn_type(...)\n","        else:\n","            try:\n","                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n","            except KeyError:\n","                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n","                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n","            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n","        self.decoder = nn.Linear(nhid, ntoken)\n","\n","        self.init_weights()\n","\n","        self.rnn_type = rnn_type\n","        self.nhid = nhid\n","        self.nlayers = nlayers\n","\n","    def init_weights(self):\n","        initrange = 0.1\n","        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n","        nn.init.zeros_(self.decoder.weight)\n","        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n","\n","    def forward(self, input, hidden):\n","        emb = self.drop(self.encoder(input))\n","        output, hidden = self.rnn(emb, hidden)\n","        output = self.drop(output)\n","        decoded = self.decoder(output)\n","        decoded = decoded.view(-1, self.ntoken)\n","        return F.log_softmax(decoded, dim=1), hidden\n","\n","    def init_hidden(self, bsz):\n","        weight = next(self.parameters())\n","        if self.rnn_type == 'LSTM':\n","            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n","                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n","        else:\n","            return weight.new_zeros(self.nlayers, bsz, self.nhid)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"dDQTvkdJdZOk","executionInfo":{"status":"ok","timestamp":1671339536278,"user_tz":-540,"elapsed":1087,"user":{"displayName":"Ïù¥Îã§Ïõê","userId":"11923633709773630529"}}},"source":["import time\n","import math\n","import os\n","import torch\n","import torch.nn as nn\n","\n","import easydict\n","args = easydict.EasyDict({\n","    \"data\"    : './data/wikitext-2',    # location of the data corpus\n","    \"model\"   : 'RNN_TANH',             # type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU)\n","    \"emsize\"  : 200,                    # size of word embeddings\n","    \"nhid\"    : 512,                    # number of hidden units per layer\n","    \"nlayers\" : 2,                      # number of layers\n","    \"lr\"      : 20,                     # initial learning rate\n","    \"clip\"    : 0.25,                   # gradient clipping\n","    \"epochs\"  : 6,                      # upper epoch limit\n","    \"batch_size\": 20,                   # batch size\n","    \"bptt\"    : 35,                     # sequence length\n","    \"dropout\" : 0.2,                    # dropout applied to layers (0 = no dropout)\n","    \"seed\"    : 1111,                   # random seed\n","    \"cuda\"    : True,                   # use CUDA\n","    \"log_interval\": 200,                # report interval\n","    \"save\"    : 'model.pt',             # path to save the final model\n","    \"dry_run\" : True,                   # verify the code and the model\n","\n","})\n","\n","# ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï\n","device = torch.device(\"cuda\" if args.cuda else \"cpu\")"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xnsZkQqUHb6Z"},"source":["```\n","train.txtÏùò Î¨∏Ïû•Îì§ÏùÑ word tokenization Ìï¥Î≥¥Í≥† Îã®Ïñ¥Îì§Ïùò Í∞úÏàòÎ•º ÏÑ∏Ïñ¥Î≥¥Í≤†ÏäµÎãàÎã§\n","```"]},{"cell_type":"code","metadata":{"id":"2aSB9Hk4HjyO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671339566597,"user_tz":-540,"elapsed":4796,"user":{"displayName":"Ïù¥Îã§Ïõê","userId":"11923633709773630529"}},"outputId":"104b5f30-3ddb-43c8-faed-5ca29f8dd3d1"},"source":["corpus = Corpus('./data/wikitext-2')\n","ntokens = len(corpus.dictionary)\n","print(ntokens)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["33278\n"]}]},{"cell_type":"markdown","metadata":{"id":"OBkvRpEcKEvd"},"source":["```\n","Ïù¥Ï†Ñ Í≥ºÏ†úÏóê ÏÇ¨Ïö©Îêú embedding dimensionÏùò ÌÅ¨Í∏∞Îäî 200Ïù¥ÎØÄÎ°ú word embeddingÏóê ÏÇ¨Ïö©Îêú parameterÏùò ÏàòÎäî 33278 x 200 (6,655,600Í∞ú)ÏûÖÎãàÎã§.\n","\n","Í∑∏Î†áÎã§Î©¥, RNN Î™®Îç∏Ïóê ÏÇ¨Ïö©ÎêòÎäî weightÏùò parameter Í∞úÏàòÎäî Î™áÍ∞úÏù∏ÏßÄ Í∞ÑÎã®Ìïú Ìï®ÏàòÎ•º Ïù¥Ïö©Ìï¥ ÌôïÏù∏Ìï¥Î≥¥Í≤†ÏäµÎãàÎã§\n","```"]},{"cell_type":"code","metadata":{"id":"rGjG2j-fKbJu","executionInfo":{"status":"ok","timestamp":1671339579204,"user_tz":-540,"elapsed":452,"user":{"displayName":"Ïù¥Îã§Ïõê","userId":"11923633709773630529"}}},"source":["model = RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"yTiKccVXLrvx","executionInfo":{"status":"ok","timestamp":1671339581300,"user_tz":-540,"elapsed":3,"user":{"displayName":"Ïù¥Îã§Ïõê","userId":"11923633709773630529"}}},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"FvRPOxLCLByf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671339597437,"user_tz":-540,"elapsed":605,"user":{"displayName":"Ïù¥Îã§Ïõê","userId":"11923633709773630529"}},"outputId":"102bbad3-a15b-4bd1-eaa7-316dbf26c312"},"source":["print(f\"Word embedding parameter Í∞úÏàò: {count_parameters(model.encoder)}\")\n","print(f\"RNN parameter Í∞úÏàò: {count_parameters(model.rnn)}\")"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Word embedding parameter Í∞úÏàò: 6655600\n","RNN parameter Í∞úÏàò: 890880\n"]}]},{"cell_type":"markdown","metadata":{"id":"nlUFvgTNM1Od"},"source":["```\n","üí° RNN parameter, Word embedding parameter Í∞úÏàòÎ•º ÎπÑÍµêÌï¥Î≥¥Î©¥ word embedding parameterÏùò Í∞úÏàòÍ∞Ä RNN Î™®Îç∏Ïùò parameterÎ≥¥Îã§ ÏïïÎèÑÏ†ÅÏúºÎ°ú ÎßéÏäµÎãàÎã§.\n","\n","word embeddingÏùÑ ÏÇ¨Ïö©ÌïòÎäî Í≤ΩÏö∞ trainingÏóê ÏÇ¨Ïö©ÎêòÎäî text fileÏùò ÌÅ¨Í∏∞Í∞Ä Ïª§ÏßàÏàòÎ°ù word embedding parameterÎäî \n","Îçî Ïª§ÏßÄÍ≤å ÎêòÍ≥† Ï†ÑÏ≤¥ parameter ÎåÄÎπÑ word embeddingÏù¥ Ï∞®ÏßÄÌïòÎäî ÎπÑÏ§ëÏùÄ Îß§Ïö∞ ÎÜíÏïÑÏßëÎãàÎã§.\n","```"]},{"cell_type":"markdown","metadata":{"id":"o7WfyYBrPpca"},"source":["```\n","‚ú® Ïù¥Îü∞ parameter ÎπÑÏ§ëÏùò ÎπÑÎåÄÏπ≠ÏÑ±ÏùÑ Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌï¥ Ï≤òÏùåÏóêÎäî character-level tokenization Î∞©Î≤ïÏù¥ Ï£ºÎ™©ÏùÑ Î∞õÏïòÏäµÎãàÎã§. \n","Îßê Í∑∏ÎåÄÎ°ú ÌïòÎÇòÏùò Í∏ÄÏûêÎ•º Í∏∞Ï§ÄÏúºÎ°ú tokenizationÏùÑ ÌïòÎäîÍ±¥Îç∞Ïöî.\n","Ïù¥Ï†Ñ ÏòàÏãúÎ•º character Í∏∞Î∞ò tokenizationÏùÑ ÌïòÎ©¥ Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§.\n","\n","\"I have a meal\" -> ['I', 'h', 'a', 'v', 'e', 'a', 'm', 'e', 'a', 'l']\n","\"ÎÇòÎäî Î∞•ÏùÑ Î®πÎäîÎã§\" -> ['ÎÇò', 'Îäî', 'Î∞•', 'ÏùÑ', 'Î®π', 'Îäî', 'Îã§']\n","\n","Í∑∏Îü¨ÎÇò, character Í∏∞Î∞ò tokenization Ïó≠Ïãú ÏßÄÎÇòÏπòÍ≤å Í∏¥ sequence Í∏∏Ïù¥, ÏÑ±Îä• Ï†ÄÌïò Îì±Ïùò Î¨∏Ï†úÎ•º Í≤™ÏúºÎ©∞ \n","subword tokenizationÏù¥ Í∞ÅÍ¥ëÏùÑ Î∞õÍ≤å ÎêòÏóàÏäµÎãàÎã§.\n","\n","‚ùì word tokenization Í∏∞Î≤ïÍ≥º Íµ¨Î∂ÑÎêòÎäî subword tokenizationÏùò Ïû•Ï†êÏùÑ ÌïòÎÇòÎßå Îçî ÏÉùÍ∞ÅÌï¥Î≥ºÍπåÏöî?\n","\n","subword tokenizationÏùò Ïû•Ï†êÏùÄ Out-of-vocabulary (OOV) Î¨∏Ï†úÏóêÏÑú ÏÉÅÎåÄÏ†ÅÏúºÎ°ú ÏûêÏú†Î°≠Îã§Îäî Í≤ÉÏûÖÎãàÎã§.\n","\n","ÏùºÎ∞òÏ†ÅÏúºÎ°ú subwordÎì§ÏùÄ ÏµúÏÜå Ï≤†Ïûê Îã®ÏúÑÏóêÏÑú ÌïòÎÇòÏî© Îçî Í∏¥ subwordÎ•º Ï∂îÍ∞ÄÌïòÎäî Î∞©ÏãùÏúºÎ°ú ÎßåÎì§Ïñ¥ÏßëÎãàÎã§.\n","\n","ÏòàÎ•º Îì§Ïñ¥, ÏòÅÏñ¥Ïùò Í≤ΩÏö∞ a~zÏùò ÏïåÌååÎ≤≥Î∂ÄÌÑ∞ ÏãúÏûëÌï¥ÏÑú ÎëêÍ∏ÄÏûê, ÏÑ∏Í∏ÄÏûê, ÎÑ§Í∏ÄÏûê subword Îì±ÏúºÎ°ú ÌôïÏû•Ìï¥ÎÇòÍ∞ÄÎ©∞ \n","subwordÎ•º Ï∂îÍ∞ÄÌï¥ Îã®Ïñ¥Î•º Íµ¨ÏÑ±ÌïòÍ≥† Ïù¥Î•º Î∞îÌÉïÏúºÎ°ú subword tokenizationÏùÑ ÏàòÌñâÌïòÍ∏∞ ÎïåÎ¨∏Ïóê Îã§Î•∏ Ïñ∏Ïñ¥Î•º tokenizationÌïòÏßÄ ÏïäÎäîÎã§Î©¥ \n","OOV Î¨∏Ï†úÏóêÏÑú ÏûêÏú†Î°≠Îã§Í≥† Î≥º Ïàò ÏûàÏäµÎãàÎã§.\n","\n","ÎåÄÌëúÏ†ÅÏù∏ subword tokenizationÏóê ÏÇ¨Ïö©ÎêòÎäî algorithm Ï§ë ÌïòÎÇòÏù∏ byte pair encodingÏùÄ ÏÑ†ÌÉù Í≥ºÏ†ú 3ÏóêÏÑú ÏÇ¥Ìé¥Î≥º Ïàò ÏûàÏúºÎãà Ï∞∏Í≥†Ìï¥Ï£ºÏÑ∏Ïöî !\n","\n","‚ú® Í∑∏Îüº Ïù¥Ï†úÎ∂ÄÌÑ∞ BERT Î™®Îç∏ÏóêÏÑú ÏÇ¨Ïö©Ìïú subword tokenization algorithmÏùÑ Ïù¥Ïö©Ìï¥ language modeling taskÎ•º ÏàòÌñâÌï¥Î≥¥Í≤†ÏäµÎãàÎã§. \n","subword tokenizerÎäî transformers ÎùºÏù¥Î∏åÎü¨Î¶¨Î•º Ïù¥Ïö©Ìï¥ ÏâΩÍ≤å Î∂àÎü¨Ïò¨ Ïàò ÏûàÏäµÎãàÎã§.\n","```\n","\n","(Ï∞∏Í≥†5: [Huggingface: subword tokenization](https://huggingface.co/transformers/tokenizer_summary.html#subword-tokenization))\n"]},{"cell_type":"code","metadata":{"id":"WZKyn3PKUuBg","executionInfo":{"status":"ok","timestamp":1671339916998,"user_tz":-540,"elapsed":11238,"user":{"displayName":"Ïù¥Îã§Ïõê","userId":"11923633709773630529"}},"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["7c49a7683e6f4f559c701d7640924afa","55c6f4db2cdc4a59a0d546ee93c16d9b","ff6ed305311f4ea096ed2abe2d421a34","fb178ef8a31f48209033d25deca06ea8","57bc324389ca446cb222d50b1bb4f2b9","cd648c27d8a9497d9f7c96c3ff70297b","a74576bb5f3b43a4b038e17643bf3b3c","7ad2fd0274dd4d0982be8c1aebde5a3a","3eb1c0369ebd421484dcc22d4d8e9a0d","e40ee12c9299448086695fb2c8d182ee","53a520ec6f8c40b097a2e6dbff9dca77","b9244bdfc67947cca4a5bde93eedc672","bfe9d98cb03b4feea392a182a0e5f79c","556d49f369e84355909b800ca3f2d60b","92d3b6653ef844f28c75da2b436913df","6ed85b6918384d79b4aa944e4eb0ba1d","ee003584886443beabcda3e3dd12fba2","a38f6df0c3824184a7b133d9b3e90c06","4080c3d775e34c8d92b7d0e3fcc44e2b","d4413acd32c148b384cb2b5bb4a30cf4","1ae5d00f82aa4179b1b539205adb4045","02b56caae6ff403c816f32b854753bca","06f6fc03e95b4257857c7b5e84b30ef1","db28a66bb471415ba7f2b4a57f3a506d","82b75640d8a149a9aaa764b1b6424d31","f12953a685e647808a209960fc81d2ac","ce49144862f44c6f9ced7c3994fa66ad","b5d4171ef3f54360adaea48a9f2c0c45","7859eb3f3c4946aab87b3fc436d406e9","01501971bb4a4514b3e5017828eb031c","bfa30fa76d5d4b45bf2a23e65c81364c","a76ab16665de47f4b0648dd0345f29ad","d4ae85dbb3c042bdb7a033359c921ae3"]},"outputId":"6b271205-7782-4eba-f801-94127da4fe3b"},"source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"],"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c49a7683e6f4f559c701d7640924afa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9244bdfc67947cca4a5bde93eedc672"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06f6fc03e95b4257857c7b5e84b30ef1"}},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"rwubp25xVUt2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671339916999,"user_tz":-540,"elapsed":13,"user":{"displayName":"Ïù¥Îã§Ïõê","userId":"11923633709773630529"}},"outputId":"cfcd122c-f2b8-48fc-970f-451b755a6bc3"},"source":["# subword tokenization ÏòàÏãú\n","print(tokenizer.tokenize('Natural language expert training course'))\n","print(tokenizer.tokenize('Goorm X KAIST'))"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["['Natural', 'language', 'expert', 'training', 'course']\n","['Go', '##orm', 'X', 'K', '##A', '##IS', '##T']\n"]}]},{"cell_type":"code","metadata":{"id":"vLfVolP3UKos","executionInfo":{"status":"ok","timestamp":1671339917000,"user_tz":-540,"elapsed":10,"user":{"displayName":"Ïù¥Îã§Ïõê","userId":"11923633709773630529"}}},"source":["class Corpus(object):\n","    def __init__(self, path):\n","        self.dictionary = Dictionary()\n","        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n","        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n","        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n","\n","    def tokenize(self, path):\n","        assert os.path.exists(path)\n","        # Add words to the dictionary\n","        with open(path, 'r', encoding=\"utf8\") as f:\n","            for line in f:\n","                words = tokenizer.tokenize(line.strip()) + ['<eos>']\n","                for word in words:\n","                    self.dictionary.add_word(word)\n","\n","        # Tokenize file content\n","        with open(path, 'r', encoding=\"utf8\") as f:\n","            idss = []\n","            for line in f:\n","                words = tokenizer.tokenize(line.strip()) + ['<eos>']\n","                ids = []\n","                for word in words:\n","                    ids.append(self.dictionary.word2idx[word])\n","                idss.append(torch.tensor(ids).type(torch.int64))\n","            ids = torch.cat(idss)\n","\n","        return ids"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BvtarB2DYiLb"},"source":["```\n","Í∑∏Îü¨Î©¥ Ïù¥Ï†ú Îã§Ïãú Î™®Îç∏ÏùÑ ÏÑ†Ïñ∏ÌïòÍ≥† parameterÏùò Í∞úÏàòÎ•º ÏÇ¥Ìé¥Î≥¥Í≤†ÏäµÎãàÎã§.\n","```"]},{"cell_type":"code","metadata":{"id":"wLKqis0hY5Or","executionInfo":{"status":"ok","timestamp":1671340015721,"user_tz":-540,"elapsed":75017,"user":{"displayName":"Ïù¥Îã§Ïõê","userId":"11923633709773630529"}}},"source":["subword_corpus = Corpus('./data/wikitext-2')\n","ntokens = len(subword_corpus.dictionary)\n","subwordmodel = RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"_RB4DQ-QZCBd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671194205158,"user_tz":-540,"elapsed":18,"user":{"displayName":"Ïù¥Îã§Ïõê","userId":"11923633709773630529"}},"outputId":"f2fba98c-47ee-4f3b-af6a-d4cd5dab52c3"},"source":["print(f\"Word embedding parameter Í∞úÏàò: {count_parameters(subwordmodel.encoder)}\")\n","print(f\"RNN parameter Í∞úÏàò: {count_parameters(subwordmodel.rnn)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Word embedding parameter Í∞úÏàò: 4619000\n","RNN parameter Í∞úÏàò: 890880\n"]}]},{"cell_type":"markdown","metadata":{"id":"q_5y4M6k1HR6"},"source":["```\n","Ïù¥Ï†ÑÏóê ÎπÑÌï¥ embedding parameter Í∞úÏàòÎäî ÌôïÏó∞Ìûà Ï§ÑÏñ¥Îì§ÏóàÏäµÎãàÎã§.\n","6,655,600Í∞ú -> 4,619,000Í∞ú\n","\n","Í∑∏Îü¨Î©¥ Ïù¥Ï†ú subword Í∏∞Î∞òÏùò Ïñ∏Ïñ¥ Î™®Îç∏ ÏÑ±Îä•ÏùÑ ÏÇ¥Ìé¥Î≥¥Í≤†ÏäµÎãàÎã§.\n","```"]},{"cell_type":"code","metadata":{"id":"JlSdSCKvd23M","executionInfo":{"status":"ok","timestamp":1671340631807,"user_tz":-540,"elapsed":7609,"user":{"displayName":"Ïù¥Îã§Ïõê","userId":"11923633709773630529"}}},"source":["###############################################################################\n","# Load data\n","###############################################################################\n","\n","# Starting from sequential data, batchify arranges the dataset into columns.\n","# For instance, with the alphabet as the sequence and batch size 4, we'd get\n","# ‚îå a g m s ‚îê\n","# ‚îÇ b h n t ‚îÇ\n","# ‚îÇ c i o u ‚îÇ\n","# ‚îÇ d j p v ‚îÇ\n","# ‚îÇ e k q w ‚îÇ\n","# ‚îî f l r x ‚îò.\n","# These columns are treated as independent by the model, which means that the\n","# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n","# batch processing.\n","\n","def batchify(data, bsz):\n","    # Work out how cleanly we can divide the dataset into bsz parts.\n","    nbatch = data.size(0) // bsz\n","    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n","    data = data.narrow(0, 0, nbatch * bsz)\n","    # Evenly divide the data across the bsz batches.\n","    data = data.view(bsz, -1).t().contiguous()\n","    return data.to(device)\n","\n","eval_batch_size = 10\n","train_data = batchify(subword_corpus.train, args.batch_size)\n","val_data = batchify(subword_corpus.valid, eval_batch_size)\n","test_data = batchify(subword_corpus.test, eval_batch_size)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"R7iRv2rHe959","executionInfo":{"status":"ok","timestamp":1671340950827,"user_tz":-540,"elapsed":4062,"user":{"displayName":"Ïù¥Îã§Ïõê","userId":"11923633709773630529"}}},"source":["###############################################################################\n","# Build the model\n","###############################################################################\n","\n","model = RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout).to(device)\n","criterion = nn.NLLLoss()"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"HiLaw_Bte_nJ","executionInfo":{"status":"ok","timestamp":1671340999349,"user_tz":-540,"elapsed":456,"user":{"displayName":"Ïù¥Îã§Ïõê","userId":"11923633709773630529"}}},"source":["###############################################################################\n","# Training code1 - define functions\n","###############################################################################\n","\n","def repackage_hidden(h):\n","    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n","\n","    if isinstance(h, torch.Tensor):\n","        return h.detach()\n","    else:\n","        return tuple(repackage_hidden(v) for v in h)\n","\n","\n","# get_batch subdivides the source data into chunks of length args.bptt.\n","# If source is equal to the example output of the batchify function, with\n","# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n","# ‚îå a g m s ‚îê ‚îå b h n t ‚îê\n","# ‚îî b h n t ‚îò ‚îî c i o u ‚îò\n","# Note that despite the name of the function, the subdivison of data is not\n","# done along the batch dimension (i.e. dimension 1), since that was handled\n","# by the batchify function. The chunks are along dimension 0, corresponding\n","# to the seq_len dimension in the LSTM.\n","\n","def get_batch(source, i):\n","    seq_len = min(args.bptt, len(source) - 1 - i)\n","    data = source[i:i+seq_len]\n","    target = source[i+1:i+1+seq_len].view(-1)\n","    return data, target\n","\n","\n","def evaluate(data_source):\n","    # Turn on evaluation mode which disables dropout.\n","    model.eval()\n","    total_loss = 0.\n","    ntokens = len(subword_corpus.dictionary)\n","    hidden = model.init_hidden(eval_batch_size)\n","    with torch.no_grad():\n","        for i in range(0, data_source.size(0) - 1, args.bptt):\n","            data, targets = get_batch(data_source, i)\n","            output, hidden = model(data, hidden)\n","            hidden = repackage_hidden(hidden)\n","            total_loss += len(data) * criterion(output, targets).item()\n","    return total_loss / (len(data_source) - 1)\n","\n","\n","def train():\n","    # Turn on training mode which enables dropout.\n","    model.train()\n","    total_loss = 0.\n","    start_time = time.time()\n","    ntokens = len(subword_corpus.dictionary)\n","    hidden = model.init_hidden(args.batch_size)\n","    for batch, i in enumerate(range(0, train_data.size(0) - 1, args.bptt)):\n","        data, targets = get_batch(train_data, i)\n","        # Starting each batch, we detach the hidden state from how it was previously produced.\n","        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n","        model.zero_grad()\n","\n","        hidden = repackage_hidden(hidden)\n","        output, hidden = model(data, hidden)\n","\n","        loss = criterion(output, targets)\n","        loss.backward()\n","\n","        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n","        for p in model.parameters():\n","            p.data.add_(p.grad, alpha=-lr)\n","\n","        total_loss += loss.item()\n","\n","        if batch % args.log_interval == 0 and batch > 0:\n","            cur_loss = total_loss / args.log_interval\n","            elapsed = time.time() - start_time\n","            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n","                    'loss {:5.2f} | ppl {:8.2f}'.format(\n","                epoch, batch, len(train_data) // args.bptt, lr,\n","                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n","            total_loss = 0\n","            start_time = time.time()\n","        if args.dry_run:\n","            break"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"A6m-cdbm7PvS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671341245904,"user_tz":-540,"elapsed":25424,"user":{"displayName":"Ïù¥Îã§Ïõê","userId":"11923633709773630529"}},"outputId":"62141a09-9ef8-43e4-8d96-5b578f3f822d"},"source":["###############################################################################\n","# Training code2 - run \n","###############################################################################\n","\n","# Loop over epochs.\n","lr = args.lr\n","best_val_loss = None\n","\n","# At any point you can hit Ctrl + C to break out of training early.\n","try:\n","    for epoch in range(1, args.epochs+1):\n","        epoch_start_time = time.time()\n","        train()\n","        val_loss = evaluate(val_data)\n","        print('-' * 89)\n","        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n","                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n","                                           val_loss, math.exp(val_loss)))\n","        print('-' * 89)\n","        # Save the model if the validation loss is the best we've seen so far.\n","        if not best_val_loss or val_loss < best_val_loss:\n","            with open(args.save, 'wb') as f:\n","                torch.save(model, f)\n","            best_val_loss = val_loss\n","        else:\n","            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n","            lr /= 4.0\n","except KeyboardInterrupt:\n","    print('-' * 89)\n","    print('Exiting from training early')\n","\n","# Load the best saved model.\n","with open(args.save, 'rb') as f:\n","    model = torch.load(f)\n","    # after load the rnn params are not a continuous chunk of memory\n","    # this makes them a continuous chunk, and will speed up forward pass\n","    # Currently, only rnn model supports flatten_parameters function.\n","    if args.model in ['RNN_TANH', 'RNN_RELU', 'LSTM', 'GRU']:\n","        model.rnn.flatten_parameters()\n","\n","# Run on test data.\n","test_loss = evaluate(test_data)\n","print('=' * 89)\n","print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n","    test_loss, math.exp(test_loss)))\n","print('=' * 89)"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["-----------------------------------------------------------------------------------------\n","| end of epoch   1 | time:  4.07s | valid loss  9.18 | valid ppl  9666.74\n","-----------------------------------------------------------------------------------------\n","-----------------------------------------------------------------------------------------\n","| end of epoch   2 | time:  3.00s | valid loss 12.36 | valid ppl 232951.82\n","-----------------------------------------------------------------------------------------\n","-----------------------------------------------------------------------------------------\n","| end of epoch   3 | time:  2.93s | valid loss 10.72 | valid ppl 45376.88\n","-----------------------------------------------------------------------------------------\n","-----------------------------------------------------------------------------------------\n","| end of epoch   4 | time:  2.95s | valid loss  9.43 | valid ppl 12452.92\n","-----------------------------------------------------------------------------------------\n","-----------------------------------------------------------------------------------------\n","| end of epoch   5 | time:  2.99s | valid loss  8.95 | valid ppl  7690.68\n","-----------------------------------------------------------------------------------------\n","-----------------------------------------------------------------------------------------\n","| end of epoch   6 | time:  2.95s | valid loss  8.59 | valid ppl  5380.83\n","-----------------------------------------------------------------------------------------\n","=========================================================================================\n","| End of training | test loss  8.42 | test ppl  4545.81\n","=========================================================================================\n"]}]},{"cell_type":"markdown","metadata":{"id":"gpv4N8wl4w8p"},"source":["### 4. ÌïôÏäµÌïú Ïñ∏Ïñ¥ Î™®Îç∏Î°ú Î¨∏Ïû• ÏÉùÏÑ±\n","\n","\n","*   ÌïôÏäµÏù¥ ÏôÑÎ£åÎêú Î™®Îç∏ÏùÑ Î∂àÎü¨ÏôÄ random Ìïú Îã®Ïñ¥Î•º input ÏúºÎ°ú ÎÑ£Ïñ¥Ï§Ä ÌõÑ Ï†ïÌï¥ÏßÑ Í∞úÏàòÏùò Îã®Ïñ¥Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§.\n","*   ÏÉùÏÑ±Ìïú Î¨∏Ïû•ÏùÑ decode ÌïòÏó¨ (Ï¶â, idx2word Î•º Ïù¥Ïö©Ìï¥ id Î•º word Î°ú Î≥ÄÌôòÌïòÏó¨) generate.txt ÌååÏùºÏóê Ï†ÄÏû•Ìï©ÎãàÎã§.\n","\n"]},{"cell_type":"code","metadata":{"id":"GQjeaKjO4JAh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671341440791,"user_tz":-540,"elapsed":2569,"user":{"displayName":"Ïù¥Îã§Ïõê","userId":"11923633709773630529"}},"outputId":"26156d9b-3840-4e12-fad6-7df94b4f8688"},"source":["###############################################################################\n","# Language Modeling on Wikitext-2\n","#\n","# This file generates new sentences sampled from the language model\n","#\n","###############################################################################\n","\n","import torch\n","\n","# Model parameters.\n","test_args = easydict.EasyDict({\n","    \"data\"      : './data/wikitext-2',  # location of data corpus\n","    \"checkpoint\": './model.pt',         # model checkpoint to use\n","    \"outf\"      : 'generate.txt',       # output file for generated text\n","    \"words\"     : 1000,                 # number of words to generate\n","    \"seed\"      : 1111,                 # random seed\n","    \"cuda\"      : True,                 # use CUDA\n","    \"temperature\": 1.0,                 # temperature - higher will increase diversity\n","    \"log_interval\": 100                 # reporting interval\n","})\n","\n","# Set the random seed manually for reproducibility.\n","torch.manual_seed(test_args.seed)\n","if torch.cuda.is_available():\n","    if not test_args.cuda:\n","        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n","\n","device = torch.device(\"cuda\" if test_args.cuda else \"cpu\")\n","\n","if test_args.temperature < 1e-3:\n","    parser.error(\"--temperature has to be greater or equal 1e-3\")\n","\n","with open(test_args.checkpoint, 'rb') as f:\n","    model = torch.load(f).to(device)\n","model.eval()\n","\n","# corpus = Corpus(test_args.data)\n","# ntokens = len(subword_corpus.dictionary)\n","\n","hidden = model.init_hidden(1)\n","input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n","\n","with open(test_args.outf, 'w') as outf:\n","    with torch.no_grad():  # no tracking history\n","        for i in range(test_args.words):\n","            output, hidden = model(input, hidden)\n","            word_weights = output.squeeze().div(test_args.temperature).exp().cpu()\n","            word_idx = torch.multinomial(word_weights, 1)[0]\n","            input.fill_(word_idx)\n","\n","            word = subword_corpus.dictionary.idx2word[word_idx]\n","\n","            outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n","\n","            if i % test_args.log_interval == 0:\n","                print('| Generated {}/{} words'.format(i, test_args.words))"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["| Generated 0/1000 words\n","| Generated 100/1000 words\n","| Generated 200/1000 words\n","| Generated 300/1000 words\n","| Generated 400/1000 words\n","| Generated 500/1000 words\n","| Generated 600/1000 words\n","| Generated 700/1000 words\n","| Generated 800/1000 words\n","| Generated 900/1000 words\n"]}]},{"cell_type":"code","source":["import time\n","a = 10\n","while a:\n","    a = a+1\n","    time.sleep(100)"],"metadata":{"id":"YETna_73VjhF","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1671196708317,"user_tz":-540,"elapsed":2475226,"user":{"displayName":"Ïù¥Îã§Ïõê","userId":"11923633709773630529"}},"outputId":"7dc4be07-25ed-41fd-a37e-afd7c68bd40f"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-d69c812fe78d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"dqLfZB-1wHHK"},"source":["## Reference\n","[Pytorch Language Model](https://github.com/pytorch/examples/tree/master/word_language_model)\n"]}]}