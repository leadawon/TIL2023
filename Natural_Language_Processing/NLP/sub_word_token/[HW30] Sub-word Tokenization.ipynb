{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"markdown","metadata":{"id":"uOnz8OxdbN3y"},"source":["# Tokenization\n"]},{"cell_type":"markdown","metadata":{"id":"TZb_LOZr7XF_"},"source":["### Data Preparation\n","\n","\n","*   ë°ì´í„°(train, dev, test)ê°€ ì €ì¥ëœ ë“œë¼ì´ë¸Œë¥¼ ë§ˆìš´íŠ¸í•˜ì—¬ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆëŠ” ê²½ë¡œë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.\n","\n"]},{"cell_type":"code","metadata":{"id":"wkCrAHWVmUck","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671194120908,"user_tz":-540,"elapsed":2862,"user":{"displayName":"ì´ë‹¤ì›","userId":"11923633709773630529"}},"outputId":"2ef9bcd6-994d-4741-d5f3-a2d2967e5e30"},"source":["import os, sys\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","\n","# í˜„ì¬ íŒŒì¼ ê²½ë¡œì— ë§ê²Œ ì„¤ì •í•´ì£¼ì„¸ìš”.\n","os.chdir('/content/drive/MyDrive/WARNING_PRIVATE_FOLDER/TIL2023/Natural_Language_Processing/NLP/sub_word_token/')"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NSVysy6N4QhW","executionInfo":{"status":"ok","timestamp":1671194123472,"user_tz":-540,"elapsed":2569,"user":{"displayName":"ì´ë‹¤ì›","userId":"11923633709773630529"}},"outputId":"7460be37-9cdc-431b-c41b-c02c3cbf1937"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"e9yYsxMfyZsi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671194123473,"user_tz":-540,"elapsed":15,"user":{"displayName":"ì´ë‹¤ì›","userId":"11923633709773630529"}},"outputId":"81b59f77-02fb-4915-d7f2-ec59e1bafef4"},"source":["# í˜„ì¬ ê²½ë¡œ í™•ì¸\n","!pwd"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/WARNING_PRIVATE_FOLDER/TIL2023/Natural_Language_Processing/NLP/sub_word_token\n","/content/drive/My Drive/WARNING_PRIVATE_FOLDER/TIL2023/Natural_Language_Processing/NLP/sub_word_token\n"]}]},{"cell_type":"code","metadata":{"id":"ceOZZ8UOU4or","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671194126482,"user_tz":-540,"elapsed":3017,"user":{"displayName":"ì´ë‹¤ì›","userId":"11923633709773630529"}},"outputId":"6a2c16d4-e474-4181-d786-14278d427fcf"},"source":["# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n","!pip install transformers"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n"]}]},{"cell_type":"markdown","metadata":{"id":"NSNON1TAbj2i"},"source":["### Introduction\n","\n","\n","* ë³¸ ê³¼ì œì˜ ëª©ì ì€ Subword tokenizationì˜ í•„ìš”ì„±ì„ ì§ì ‘ ëŠê»´ë³´ëŠ” ê²ƒì…ë‹ˆë‹¤.\n","* Subword tokenization ê¸°ë°˜ language modelì„ êµ¬í˜„í•˜ë©´ì„œ ì´ì „ ê³¼ì œì˜ Word-level language modelê³¼ ë¹„êµí•´ë³´ëŠ” ì‹œê°„ì„ ê°–ê² ìŠµë‹ˆë‹¤. ì¶”ê°€ì ìœ¼ë¡œ RNNì„ LSTMìœ¼ë¡œ ë³€ê²½í–ˆì„ ë•Œì˜ ì„±ëŠ¥ ì°¨ì´ì— ëŒ€í•´ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n","*   Subword-level language modelì„ êµ¬í˜„í•˜ê³ , ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ê°€ê³µí•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•œ í›„ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸ì„ ì´ìš©í•´ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n"]},{"cell_type":"markdown","metadata":{"id":"_xzSMF9RJyzg"},"source":["\n","```\n","ğŸ’¡ SubwordëŠ” ë¬´ì—‡ì¸ê°€ìš”?\n","\n","SubwordëŠ” í•˜ë‚˜ì˜ ë‹¨ì–´ë¥¼ ì—¬ëŸ¬ê°œì˜ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í–ˆì„ ë•Œ í•˜ë‚˜ì˜ ë‹¨ìœ„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. \"subword\"ë¥¼ subword ë‹¨ìœ„ë¡œ ë‚˜íƒ€ë‚¸ í•˜ë‚˜ì˜ ì˜ˆì‹œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n","\n","\"sub\" + \"word\"\n","\n","subë¼ëŠ” ì ‘ë‘ì‚¬ì™€ wordë¼ê³  í•˜ëŠ” ì–´ê·¼ìœ¼ë¡œ ë‚˜ëˆ„ì–´ \"subword\"ë¼ê³  í•˜ëŠ” wordë¥¼ 2ê°œì˜ subwordë¡œ ë‚˜íƒ€ëƒˆìŠµë‹ˆë‹¤.\n","\n","ì´ì™¸ì—ë„ ë‹¤ì–‘í•œ í˜•íƒœì˜ subwordë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (e.g., \"su\" + \"bword\", \"s\" + \"ubword\", \"subwor\" + \"d\")\n","```"]},{"cell_type":"markdown","metadata":{"id":"zrgY-yRfVJON"},"source":["```\n","ğŸ’¡ tokenizationì€ ë¬´ì—‡ì¸ê°€ìš”?\n","\n","tokenizationì€ ì£¼ì–´ì§„ ì…ë ¥ ë°ì´í„°ë¥¼ ìì—°ì–´ì²˜ë¦¬ ëª¨ë¸ì´ ì¸ì‹í•  ìˆ˜ ìˆëŠ” ë‹¨ìœ„ë¡œ ë³€í™˜í•´ì£¼ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. \n","\n","ğŸ’¡ word tokenizationì€ìš”?\n","\n","word tokenizationì˜ ê²½ìš° \"ë‹¨ì–´\"ê°€ ìì—°ì–´ì²˜ë¦¬ ëª¨ë¸ì´ ì¸ì‹í•˜ëŠ” ë‹¨ìœ„ê°€ ë©ë‹ˆë‹¤.\n","\"I have a meal\"ì´ë¼ê³  í•˜ëŠ” ë¬¸ì¥ì„ ê°€ì§€ê³  word tokenizationì„ í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. \n","\n","- ['I', 'have', 'a', 'meal']\n","\n","ì˜ì–´ì˜ ê²½ìš° ëŒ€ë¶€ë¶„ spaceë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ê°€ ì •ì˜ë˜ê¸° ë•Œë¬¸ì— .split()ì„ ì´ìš©í•´ ì‰½ê²Œ word tokenizationì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n","ì˜ì–´ì—ì„œ word tokenizationì€ space tokenizationì´ë¼ê³ ë„ í•  ìˆ˜ ìˆê³ , \n","subword tokenization ì´ì „ì— ìˆ˜í–‰ë˜ëŠ” pre-tokenization ë°©ë²•ìœ¼ë¡œë„ ë§ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.\n","\n","\"ë‚˜ëŠ” ë°¥ì„ ë¨¹ëŠ”ë‹¤\"ë¼ëŠ” ë¬¸ì¥ì„ word tokenizationí•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n","- ['ë‚˜', 'ëŠ”', 'ë°¥', 'ì„', 'ë¨¹ëŠ”ë‹¤']\n","\n","í•œêµ­ì–´ì—ì„œ \"ë‹¨ì–´\"ëŠ” ê³µë°±(space)ì„ ê¸°ì¤€ìœ¼ë¡œ ì •ì˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ëŠ” í•œêµ­ì–´ê°€ ê°–ê³  ìˆëŠ” \"êµì°©ì–´\"ë¡œì„œì˜ íŠ¹ì§• ë•Œë¬¸ì…ë‹ˆë‹¤. \n","ì²´ì–¸ ë’¤ì— ì¡°ì‚¬ê°€ ë¶™ëŠ” ê²ƒì´ ëŒ€í‘œì ì¸ íŠ¹ì§•ì´ë©° ì˜ë¯¸ ë‹¨ìœ„ê°€ êµ¬ë¶„ë˜ê³  ìë¦½ì„±ì´ ìˆê¸° ë•Œë¬¸ì— ì¡°ì‚¬ëŠ” \"ë‹¨ì–´\"ì…ë‹ˆë‹¤.\n","\n","í•œêµ­ì–´ì—ì„œëŠ” pre-tokenization ë°©ë²•ìœ¼ë¡œ space tokenizationì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ í™œìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n","```\n","\n","(ì°¸ê³ 1: [êµ­ë¦½ êµ­ì–´ì›: \"ì¡°ì‚¬ëŠ” ë‹¨ì–´ì´ë‹¤\"](https://www.korean.go.kr/front/onlineQna/onlineQnaView.do?mn_id=216&qna_seq=26915#:~:text='%EC%A1%B0%EC%82%AC'%EB%8A%94%20%EC%99%84%EC%A0%84%ED%95%9C%20%EC%9E%90%EB%A6%BD%EC%84%B1%EC%9D%80,%ED%95%98%EC%97%AC%20%EB%8B%A8%EC%96%B4%EB%A1%9C%20%EC%B2%98%EB%A6%AC%ED%95%A9%EB%8B%88%EB%8B%A4.) )\n","\n","(ì°¸ê³ 2: [Huggingface: Pre-tokenization](https://huggingface.co/docs/tokenizers/python/latest/pipeline.html#pre-tokenization))\n","\n","(ì°¸ê³ 3: [Konlpy: í˜•íƒœì†Œ ë¶„ì„ê¸°](https://konlpy-ko.readthedocs.io/ko/v0.4.3/morph/))"]},{"cell_type":"markdown","metadata":{"id":"BVQgjQJdAbWh"},"source":["```\n","ğŸ’¡ ê·¸ëŸ¼ Subword tokenizationì€ ë¬´ì—‡ì¸ê°€ìš”?\n","\n","Subword tokenizaitonì€ ë§ ê·¸ëŒ€ë¡œ subword ë‹¨ìœ„ë¡œ tokenizationì„ í•œë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.\n","ë°©ê¸ˆ ì „ word tokenizationì„ ìˆ˜í–‰í–ˆë˜ ë¬¸ì¥ì„ ì´ìš©í•´ subword tokenizationì„ ìˆ˜í–‰í•œ ì˜ˆì‹œë¥¼ ë³´ê² ìŠµë‹ˆë‹¤.\n","\n","Subword tokenizationì„ ì ìš©í–ˆì„ ë•ŒëŠ” ë‹¤ìŒê³¼ ê°™ì´ tokenizationì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n","\n","Example 1\n","\n","\"I have a meal\" -> ['I', 'hav', 'e', 'a', 'me', 'al']\n","\"ë‚˜ëŠ” ë°¥ì„ ë¨¹ëŠ”ë‹¤\" -> ['ë‚˜', 'ëŠ”', 'ë°¥', 'ì„', 'ë¨¹ëŠ”', 'ë‹¤']\n","\n","word ë‹¨ìœ„ê°€ ì•„ë‹ˆë¼ ê·¸ë³´ë‹¤ ë” ì˜ê²Œ ìª¼ê°  subword ë‹¨ìœ„ë¡œ ë¬¸ì¥ì„ tokenizationí•©ë‹ˆë‹¤.\n","\n","ìœ„ì—ì„œ ë§ì”€ë“œë¦° ê²ƒê³¼ ê°™ì´ ì—¬ëŸ¬ê°€ì§€ ê²½ìš°ì˜ ìˆ˜ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n","\n","Example 2\n","\n","\"I have a meal\" -> ['I', 'ha', 've', 'a', 'mea', 'l']\n","\"ë‚˜ëŠ” ë°¥ì„ ë¨¹ëŠ”ë‹¤\" -> ['ë‚˜', 'ëŠ”', 'ë°¥', 'ì„', 'ë¨¹', 'ëŠ”ë‹¤']\n","\n","ê·¸ë ‡ì§€ë§Œ ê¸°ë³¸ì ìœ¼ë¡œ ê³µë°±ì„ ë„˜ì–´ì„  subwordë¥¼ êµ¬ì„±í•˜ì§„ ì•ŠìŠµë‹ˆë‹¤.\n","ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒê³¼ ê°™ì´ tokenizaitonì„ ìˆ˜í–‰í•˜ì§„ ì•ŠìŠµë‹ˆë‹¤.\n","\n","Example 3\n","\n","\"I have a meal\" -> ['Iha', 've', 'am', 'ea', 'l']\n","\"ë‚˜ëŠ” ë°¥ì„ ë¨¹ëŠ”ë‹¤\" -> ['ë‚˜ëŠ”ë°¥', 'ì„ë¨¹', 'ëŠ”ë‹¤']\n","```\n","\n","(ì°¸ê³ 4: [Huggingface: subword-tokenization](https://huggingface.co/transformers/tokenizer_summary.html#subword-tokenization))"]},{"cell_type":"markdown","metadata":{"id":"SPRNaFhMEK67"},"source":["```\n","ğŸ’¡ Subword tokenizationì€ ì™œ í•„ìš”í•œê°€ìš”?\n","\n","word tokenization ì½”ë“œë¥¼ ë¶ˆëŸ¬ì™€ ê·¸ í•„ìš”ì„±ì„ ìƒê°í•´ ë´…ì‹œë‹¤.\n","\n","```"]},{"cell_type":"code","metadata":{"id":"DESsQzhwGVST","executionInfo":{"status":"ok","timestamp":1671194126483,"user_tz":-540,"elapsed":18,"user":{"displayName":"ì´ë‹¤ì›","userId":"11923633709773630529"}}},"source":["import os\n","from io import open\n","import torch"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"SWd09aUBGWa9","executionInfo":{"status":"ok","timestamp":1671194126484,"user_tz":-540,"elapsed":16,"user":{"displayName":"ì´ë‹¤ì›","userId":"11923633709773630529"}}},"source":["class Dictionary(object):\n","    def __init__(self):\n","        self.word2idx = {'<unk>': 0}\n","        self.idx2word = ['<unk>']\n","\n","    def add_word(self, word):\n","        if word not in self.word2idx:\n","            self.idx2word.append(word)\n","            self.word2idx[word] = len(self.idx2word) - 1\n","        return self.word2idx[word]\n","\n","    def __len__(self):\n","        return len(self.idx2word)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"xQIyGQlfd9Os","executionInfo":{"status":"ok","timestamp":1671194126484,"user_tz":-540,"elapsed":14,"user":{"displayName":"ì´ë‹¤ì›","userId":"11923633709773630529"}}},"source":["class Corpus(object):\n","    def __init__(self, path):\n","        self.dictionary = Dictionary()\n","        \"\"\"Tokenizes a text file.\"\"\"\n","        assert os.path.exists(path)\n","        # Add words to the dictionary\n","        with open(os.path.join(path, 'train.txt'), 'r', encoding=\"utf8\") as f:\n","            for line in f:\n","                words = line.split() + ['<eos>']\n","                for word in words:\n","                    self.dictionary.add_word(word)\n","\n","        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n","        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n","        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n","\n","    def tokenize(self, path):\n","        # Tokenize file content\n","        with open(path, 'r', encoding=\"utf8\") as f:\n","            idss = []\n","            for line in f:\n","                words = line.split() + ['<eos>']\n","                ids = []\n","                for word in words:\n","                    try:\n","                        ids.append(self.dictionary.word2idx[word])\n","                    except:\n","                        print(word)\n","                        ids.append(0)\n","                idss.append(torch.tensor(ids).type(torch.int64))\n","            ids = torch.cat(idss)\n","\n","        return ids"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"a55D0z53eLzB","executionInfo":{"status":"ok","timestamp":1671194126484,"user_tz":-540,"elapsed":13,"user":{"displayName":"ì´ë‹¤ì›","userId":"11923633709773630529"}}},"source":["import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class RNNModel(nn.Module):\n","    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n","\n","    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5):\n","        super(RNNModel, self).__init__()\n","        self.ntoken = ntoken\n","        self.drop = nn.Dropout(dropout)\n","        self.encoder = nn.Embedding(ntoken, ninp)\n","        if rnn_type in ['LSTM', 'GRU']:\n","            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout) #nn.rnn_type(...)\n","        else:\n","            try:\n","                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n","            except KeyError:\n","                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n","                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n","            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n","        self.decoder = nn.Linear(nhid, ntoken)\n","\n","        self.init_weights()\n","\n","        self.rnn_type = rnn_type\n","        self.nhid = nhid\n","        self.nlayers = nlayers\n","\n","    def init_weights(self):\n","        initrange = 0.1\n","        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n","        nn.init.zeros_(self.decoder.weight)\n","        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n","\n","    def forward(self, input, hidden):\n","        emb = self.drop(self.encoder(input))\n","        output, hidden = self.rnn(emb, hidden)\n","        output = self.drop(output)\n","        decoded = self.decoder(output)\n","        decoded = decoded.view(-1, self.ntoken)\n","        return F.log_softmax(decoded, dim=1), hidden\n","\n","    def init_hidden(self, bsz):\n","        weight = next(self.parameters())\n","        if self.rnn_type == 'LSTM':\n","            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n","                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n","        else:\n","            return weight.new_zeros(self.nlayers, bsz, self.nhid)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"dDQTvkdJdZOk","executionInfo":{"status":"ok","timestamp":1671194126485,"user_tz":-540,"elapsed":13,"user":{"displayName":"ì´ë‹¤ì›","userId":"11923633709773630529"}}},"source":["import time\n","import math\n","import os\n","import torch\n","import torch.nn as nn\n","\n","import easydict\n","args = easydict.EasyDict({\n","    \"data\"    : './data/wikitext-2',    # location of the data corpus\n","    \"model\"   : 'RNN_TANH',             # type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU)\n","    \"emsize\"  : 200,                    # size of word embeddings\n","    \"nhid\"    : 512,                    # number of hidden units per layer\n","    \"nlayers\" : 2,                      # number of layers\n","    \"lr\"      : 20,                     # initial learning rate\n","    \"clip\"    : 0.25,                   # gradient clipping\n","    \"epochs\"  : 6,                      # upper epoch limit\n","    \"batch_size\": 20,                   # batch size\n","    \"bptt\"    : 35,                     # sequence length\n","    \"dropout\" : 0.2,                    # dropout applied to layers (0 = no dropout)\n","    \"seed\"    : 1111,                   # random seed\n","    \"cuda\"    : True,                   # use CUDA\n","    \"log_interval\": 200,                # report interval\n","    \"save\"    : 'model.pt',             # path to save the final model\n","    \"dry_run\" : True,                   # verify the code and the model\n","\n","})\n","\n","# ë””ë°”ì´ìŠ¤ ì„¤ì •\n","device = torch.device(\"cuda\" if args.cuda else \"cpu\")"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xnsZkQqUHb6Z"},"source":["```\n","train.txtì˜ ë¬¸ì¥ë“¤ì„ word tokenization í•´ë³´ê³  ë‹¨ì–´ë“¤ì˜ ê°œìˆ˜ë¥¼ ì„¸ì–´ë³´ê² ìŠµë‹ˆë‹¤\n","```"]},{"cell_type":"code","metadata":{"id":"2aSB9Hk4HjyO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671194128353,"user_tz":-540,"elapsed":1880,"user":{"displayName":"ì´ë‹¤ì›","userId":"11923633709773630529"}},"outputId":"d8db68e3-f33e-47ea-d5c7-c4ce3407067f"},"source":["corpus = Corpus('./data/wikitext-2')\n","ntokens = len(corpus.dictionary)\n","print(ntokens)"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["33278\n"]}]},{"cell_type":"markdown","metadata":{"id":"OBkvRpEcKEvd"},"source":["```\n","ì´ì „ ê³¼ì œì— ì‚¬ìš©ëœ embedding dimensionì˜ í¬ê¸°ëŠ” 200ì´ë¯€ë¡œ word embeddingì— ì‚¬ìš©ëœ parameterì˜ ìˆ˜ëŠ” 33278 x 200 (6,655,600ê°œ)ì…ë‹ˆë‹¤.\n","\n","ê·¸ë ‡ë‹¤ë©´, RNN ëª¨ë¸ì— ì‚¬ìš©ë˜ëŠ” weightì˜ parameter ê°œìˆ˜ëŠ” ëª‡ê°œì¸ì§€ ê°„ë‹¨í•œ í•¨ìˆ˜ë¥¼ ì´ìš©í•´ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤\n","```"]},{"cell_type":"code","metadata":{"id":"rGjG2j-fKbJu","executionInfo":{"status":"ok","timestamp":1671194128354,"user_tz":-540,"elapsed":13,"user":{"displayName":"ì´ë‹¤ì›","userId":"11923633709773630529"}}},"source":["model = RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout)"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"yTiKccVXLrvx","executionInfo":{"status":"ok","timestamp":1671194128355,"user_tz":-540,"elapsed":12,"user":{"displayName":"ì´ë‹¤ì›","userId":"11923633709773630529"}}},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"FvRPOxLCLByf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671194128356,"user_tz":-540,"elapsed":12,"user":{"displayName":"ì´ë‹¤ì›","userId":"11923633709773630529"}},"outputId":"ecd4bcd0-456c-4839-f996-a53dac0b940a"},"source":["print(f\"Word embedding parameter ê°œìˆ˜: {count_parameters(model.encoder)}\")\n","print(f\"RNN parameter ê°œìˆ˜: {count_parameters(model.rnn)}\")"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Word embedding parameter ê°œìˆ˜: 6655600\n","RNN parameter ê°œìˆ˜: 890880\n"]}]},{"cell_type":"markdown","metadata":{"id":"nlUFvgTNM1Od"},"source":["```\n","ğŸ’¡ RNN parameter, Word embedding parameter ê°œìˆ˜ë¥¼ ë¹„êµí•´ë³´ë©´ word embedding parameterì˜ ê°œìˆ˜ê°€ RNN ëª¨ë¸ì˜ parameterë³´ë‹¤ ì••ë„ì ìœ¼ë¡œ ë§ìŠµë‹ˆë‹¤.\n","\n","word embeddingì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° trainingì— ì‚¬ìš©ë˜ëŠ” text fileì˜ í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡ word embedding parameterëŠ” \n","ë” ì»¤ì§€ê²Œ ë˜ê³  ì „ì²´ parameter ëŒ€ë¹„ word embeddingì´ ì°¨ì§€í•˜ëŠ” ë¹„ì¤‘ì€ ë§¤ìš° ë†’ì•„ì§‘ë‹ˆë‹¤.\n","```"]},{"cell_type":"markdown","metadata":{"id":"o7WfyYBrPpca"},"source":["```\n","âœ¨ ì´ëŸ° parameter ë¹„ì¤‘ì˜ ë¹„ëŒ€ì¹­ì„±ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ì²˜ìŒì—ëŠ” character-level tokenization ë°©ë²•ì´ ì£¼ëª©ì„ ë°›ì•˜ìŠµë‹ˆë‹¤. \n","ë§ ê·¸ëŒ€ë¡œ í•˜ë‚˜ì˜ ê¸€ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ tokenizationì„ í•˜ëŠ”ê±´ë°ìš”.\n","ì´ì „ ì˜ˆì‹œë¥¼ character ê¸°ë°˜ tokenizationì„ í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n","\n","\"I have a meal\" -> ['I', 'h', 'a', 'v', 'e', 'a', 'm', 'e', 'a', 'l']\n","\"ë‚˜ëŠ” ë°¥ì„ ë¨¹ëŠ”ë‹¤\" -> ['ë‚˜', 'ëŠ”', 'ë°¥', 'ì„', 'ë¨¹', 'ëŠ”', 'ë‹¤']\n","\n","ê·¸ëŸ¬ë‚˜, character ê¸°ë°˜ tokenization ì—­ì‹œ ì§€ë‚˜ì¹˜ê²Œ ê¸´ sequence ê¸¸ì´, ì„±ëŠ¥ ì €í•˜ ë“±ì˜ ë¬¸ì œë¥¼ ê²ªìœ¼ë©° \n","subword tokenizationì´ ê°ê´‘ì„ ë°›ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.\n","\n","â“ word tokenization ê¸°ë²•ê³¼ êµ¬ë¶„ë˜ëŠ” subword tokenizationì˜ ì¥ì ì„ í•˜ë‚˜ë§Œ ë” ìƒê°í•´ë³¼ê¹Œìš”?\n","\n","subword tokenizationì˜ ì¥ì ì€ Out-of-vocabulary (OOV) ë¬¸ì œì—ì„œ ìƒëŒ€ì ìœ¼ë¡œ ììœ ë¡­ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.\n","\n","ì¼ë°˜ì ìœ¼ë¡œ subwordë“¤ì€ ìµœì†Œ ì² ì ë‹¨ìœ„ì—ì„œ í•˜ë‚˜ì”© ë” ê¸´ subwordë¥¼ ì¶”ê°€í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë§Œë“¤ì–´ì§‘ë‹ˆë‹¤.\n","\n","ì˜ˆë¥¼ ë“¤ì–´, ì˜ì–´ì˜ ê²½ìš° a~zì˜ ì•ŒíŒŒë²³ë¶€í„° ì‹œì‘í•´ì„œ ë‘ê¸€ì, ì„¸ê¸€ì, ë„¤ê¸€ì subword ë“±ìœ¼ë¡œ í™•ì¥í•´ë‚˜ê°€ë©° \n","subwordë¥¼ ì¶”ê°€í•´ ë‹¨ì–´ë¥¼ êµ¬ì„±í•˜ê³  ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ subword tokenizationì„ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì— ë‹¤ë¥¸ ì–¸ì–´ë¥¼ tokenizationí•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ \n","OOV ë¬¸ì œì—ì„œ ììœ ë¡­ë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n","\n","ëŒ€í‘œì ì¸ subword tokenizationì— ì‚¬ìš©ë˜ëŠ” algorithm ì¤‘ í•˜ë‚˜ì¸ byte pair encodingì€ ì„ íƒ ê³¼ì œ 3ì—ì„œ ì‚´í´ë³¼ ìˆ˜ ìˆìœ¼ë‹ˆ ì°¸ê³ í•´ì£¼ì„¸ìš” !\n","\n","âœ¨ ê·¸ëŸ¼ ì´ì œë¶€í„° BERT ëª¨ë¸ì—ì„œ ì‚¬ìš©í•œ subword tokenization algorithmì„ ì´ìš©í•´ language modeling taskë¥¼ ìˆ˜í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤. \n","subword tokenizerëŠ” transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•´ ì‰½ê²Œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n","```\n","\n","(ì°¸ê³ 5: [Huggingface: subword tokenization](https://huggingface.co/transformers/tokenizer_summary.html#subword-tokenization))\n"]},{"cell_type":"code","metadata":{"id":"WZKyn3PKUuBg","executionInfo":{"status":"ok","timestamp":1671194132877,"user_tz":-540,"elapsed":4531,"user":{"displayName":"ì´ë‹¤ì›","userId":"11923633709773630529"}}},"source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"rwubp25xVUt2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671194132878,"user_tz":-540,"elapsed":29,"user":{"displayName":"ì´ë‹¤ì›","userId":"11923633709773630529"}},"outputId":"4c4d9713-0664-4dfd-f676-37ac4249c811"},"source":["# subword tokenization ì˜ˆì‹œ\n","print(tokenizer.tokenize('Natural language expert training course'))\n","print(tokenizer.tokenize('Goorm X KAIST'))"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["['Natural', 'language', 'expert', 'training', 'course']\n","['Go', '##orm', 'X', 'K', '##A', '##IS', '##T']\n"]}]},{"cell_type":"code","metadata":{"id":"vLfVolP3UKos","executionInfo":{"status":"ok","timestamp":1671194132878,"user_tz":-540,"elapsed":25,"user":{"displayName":"ì´ë‹¤ì›","userId":"11923633709773630529"}}},"source":["class Corpus(object):\n","    def __init__(self, path):\n","        self.dictionary = Dictionary()\n","        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n","        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n","        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n","\n","    def tokenize(self, path):\n","        assert os.path.exists(path)\n","        # Add words to the dictionary\n","        with open(path, 'r', encoding=\"utf8\") as f:\n","            for line in f:\n","                words = tokenizer.tokenize(line.strip()) + ['<eos>']\n","                for word in words:\n","                    self.dictionary.add_word(word)\n","\n","        # Tokenize file content\n","        with open(path, 'r', encoding=\"utf8\") as f:\n","            idss = []\n","            for line in f:\n","                words = tokenizer.tokenize(line.strip()) + ['<eos>']\n","                ids = []\n","                for word in words:\n","                    ids.append(self.dictionary.word2idx[word])\n","                idss.append(torch.tensor(ids).type(torch.int64))\n","            ids = torch.cat(idss)\n","\n","        return ids"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BvtarB2DYiLb"},"source":["```\n","ê·¸ëŸ¬ë©´ ì´ì œ ë‹¤ì‹œ ëª¨ë¸ì„ ì„ ì–¸í•˜ê³  parameterì˜ ê°œìˆ˜ë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n","```"]},{"cell_type":"code","metadata":{"id":"wLKqis0hY5Or","executionInfo":{"status":"ok","timestamp":1671194205157,"user_tz":-540,"elapsed":72303,"user":{"displayName":"ì´ë‹¤ì›","userId":"11923633709773630529"}}},"source":["subword_corpus = Corpus('./data/wikitext-2')\n","ntokens = len(subword_corpus.dictionary)\n","subwordmodel = RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout)"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"_RB4DQ-QZCBd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671194205158,"user_tz":-540,"elapsed":18,"user":{"displayName":"ì´ë‹¤ì›","userId":"11923633709773630529"}},"outputId":"f2fba98c-47ee-4f3b-af6a-d4cd5dab52c3"},"source":["print(f\"Word embedding parameter ê°œìˆ˜: {count_parameters(subwordmodel.encoder)}\")\n","print(f\"RNN parameter ê°œìˆ˜: {count_parameters(subwordmodel.rnn)}\")"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Word embedding parameter ê°œìˆ˜: 4619000\n","RNN parameter ê°œìˆ˜: 890880\n"]}]},{"cell_type":"markdown","metadata":{"id":"q_5y4M6k1HR6"},"source":["```\n","ì´ì „ì— ë¹„í•´ embedding parameter ê°œìˆ˜ëŠ” í™•ì—°íˆ ì¤„ì–´ë“¤ì—ˆìŠµë‹ˆë‹¤.\n","6,655,600ê°œ -> 4,619,000ê°œ\n","\n","ê·¸ëŸ¬ë©´ ì´ì œ subword ê¸°ë°˜ì˜ ì–¸ì–´ ëª¨ë¸ ì„±ëŠ¥ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n","```"]},{"cell_type":"code","metadata":{"id":"JlSdSCKvd23M","executionInfo":{"status":"ok","timestamp":1671194207712,"user_tz":-540,"elapsed":2570,"user":{"displayName":"ì´ë‹¤ì›","userId":"11923633709773630529"}}},"source":["###############################################################################\n","# Load data\n","###############################################################################\n","\n","# Starting from sequential data, batchify arranges the dataset into columns.\n","# For instance, with the alphabet as the sequence and batch size 4, we'd get\n","# â”Œ a g m s â”\n","# â”‚ b h n t â”‚\n","# â”‚ c i o u â”‚\n","# â”‚ d j p v â”‚\n","# â”‚ e k q w â”‚\n","# â”” f l r x â”˜.\n","# These columns are treated as independent by the model, which means that the\n","# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n","# batch processing.\n","\n","def batchify(data, bsz):\n","    # Work out how cleanly we can divide the dataset into bsz parts.\n","    nbatch = data.size(0) // bsz\n","    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n","    data = data.narrow(0, 0, nbatch * bsz)\n","    # Evenly divide the data across the bsz batches.\n","    data = data.view(bsz, -1).t().contiguous()\n","    return data.to(device)\n","\n","eval_batch_size = 10\n","train_data = batchify(subword_corpus.train, args.batch_size)\n","val_data = batchify(subword_corpus.valid, eval_batch_size)\n","test_data = batchify(subword_corpus.test, eval_batch_size)"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"R7iRv2rHe959","executionInfo":{"status":"ok","timestamp":1671194208899,"user_tz":-540,"elapsed":1237,"user":{"displayName":"ì´ë‹¤ì›","userId":"11923633709773630529"}}},"source":["###############################################################################\n","# Build the model\n","###############################################################################\n","\n","model = RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout).to(device)\n","criterion = nn.NLLLoss()"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"HiLaw_Bte_nJ","executionInfo":{"status":"ok","timestamp":1671194208902,"user_tz":-540,"elapsed":15,"user":{"displayName":"ì´ë‹¤ì›","userId":"11923633709773630529"}}},"source":["###############################################################################\n","# Training code1 - define functions\n","###############################################################################\n","\n","def repackage_hidden(h):\n","    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n","\n","    if isinstance(h, torch.Tensor):\n","        return h.detach()\n","    else:\n","        return tuple(repackage_hidden(v) for v in h)\n","\n","\n","# get_batch subdivides the source data into chunks of length args.bptt.\n","# If source is equal to the example output of the batchify function, with\n","# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n","# â”Œ a g m s â” â”Œ b h n t â”\n","# â”” b h n t â”˜ â”” c i o u â”˜\n","# Note that despite the name of the function, the subdivison of data is not\n","# done along the batch dimension (i.e. dimension 1), since that was handled\n","# by the batchify function. The chunks are along dimension 0, corresponding\n","# to the seq_len dimension in the LSTM.\n","\n","def get_batch(source, i):\n","    seq_len = min(args.bptt, len(source) - 1 - i)\n","    data = source[i:i+seq_len]\n","    target = source[i+1:i+1+seq_len].view(-1)\n","    return data, target\n","\n","\n","def evaluate(data_source):\n","    # Turn on evaluation mode which disables dropout.\n","    model.eval()\n","    total_loss = 0.\n","    ntokens = len(subword_corpus.dictionary)\n","    hidden = model.init_hidden(eval_batch_size)\n","    with torch.no_grad():\n","        for i in range(0, data_source.size(0) - 1, args.bptt):\n","            data, targets = get_batch(data_source, i)\n","            output, hidden = model(data, hidden)\n","            hidden = repackage_hidden(hidden)\n","            total_loss += len(data) * criterion(output, targets).item()\n","    return total_loss / (len(data_source) - 1)\n","\n","\n","def train():\n","    # Turn on training mode which enables dropout.\n","    model.train()\n","    total_loss = 0.\n","    start_time = time.time()\n","    ntokens = len(subword_corpus.dictionary)\n","    hidden = model.init_hidden(args.batch_size)\n","    for batch, i in enumerate(range(0, train_data.size(0) - 1, args.bptt)):\n","        data, targets = get_batch(train_data, i)\n","        # Starting each batch, we detach the hidden state from how it was previously produced.\n","        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n","        model.zero_grad()\n","\n","        hidden = repackage_hidden(hidden)\n","        output, hidden = model(data, hidden)\n","\n","        loss = criterion(output, targets)\n","        loss.backward()\n","\n","        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n","        for p in model.parameters():\n","            p.data.add_(p.grad, alpha=-lr)\n","\n","        total_loss += loss.item()\n","\n","        if batch % args.log_interval == 0 and batch > 0:\n","            cur_loss = total_loss / args.log_interval\n","            elapsed = time.time() - start_time\n","            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n","                    'loss {:5.2f} | ppl {:8.2f}'.format(\n","                epoch, batch, len(train_data) // args.bptt, lr,\n","                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n","            total_loss = 0\n","            start_time = time.time()\n","        if args.dry_run:\n","            break"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"A6m-cdbm7PvS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671194231452,"user_tz":-540,"elapsed":22562,"user":{"displayName":"ì´ë‹¤ì›","userId":"11923633709773630529"}},"outputId":"2fd7a593-2a06-473d-89be-d60e5c6a09af"},"source":["###############################################################################\n","# Training code2 - run \n","###############################################################################\n","\n","# Loop over epochs.\n","lr = args.lr\n","best_val_loss = None\n","\n","# At any point you can hit Ctrl + C to break out of training early.\n","try:\n","    for epoch in range(1, args.epochs+1):\n","        epoch_start_time = time.time()\n","        train()\n","        val_loss = evaluate(val_data)\n","        print('-' * 89)\n","        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n","                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n","                                           val_loss, math.exp(val_loss)))\n","        print('-' * 89)\n","        # Save the model if the validation loss is the best we've seen so far.\n","        if not best_val_loss or val_loss < best_val_loss:\n","            with open(args.save, 'wb') as f:\n","                torch.save(model, f)\n","            best_val_loss = val_loss\n","        else:\n","            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n","            lr /= 4.0\n","except KeyboardInterrupt:\n","    print('-' * 89)\n","    print('Exiting from training early')\n","\n","# Load the best saved model.\n","with open(args.save, 'rb') as f:\n","    model = torch.load(f)\n","    # after load the rnn params are not a continuous chunk of memory\n","    # this makes them a continuous chunk, and will speed up forward pass\n","    # Currently, only rnn model supports flatten_parameters function.\n","    if args.model in ['RNN_TANH', 'RNN_RELU', 'LSTM', 'GRU']:\n","        model.rnn.flatten_parameters()\n","\n","# Run on test data.\n","test_loss = evaluate(test_data)\n","print('=' * 89)\n","print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n","    test_loss, math.exp(test_loss)))\n","print('=' * 89)"],"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["-----------------------------------------------------------------------------------------\n","| end of epoch   1 | time:  3.32s | valid loss  8.98 | valid ppl  7955.79\n","-----------------------------------------------------------------------------------------\n","-----------------------------------------------------------------------------------------\n","| end of epoch   2 | time:  2.97s | valid loss 12.64 | valid ppl 308226.21\n","-----------------------------------------------------------------------------------------\n","-----------------------------------------------------------------------------------------\n","| end of epoch   3 | time:  3.00s | valid loss 10.88 | valid ppl 52931.24\n","-----------------------------------------------------------------------------------------\n","-----------------------------------------------------------------------------------------\n","| end of epoch   4 | time:  3.00s | valid loss  9.37 | valid ppl 11789.07\n","-----------------------------------------------------------------------------------------\n","-----------------------------------------------------------------------------------------\n","| end of epoch   5 | time:  3.06s | valid loss  8.95 | valid ppl  7672.45\n","-----------------------------------------------------------------------------------------\n","-----------------------------------------------------------------------------------------\n","| end of epoch   6 | time:  3.04s | valid loss  8.63 | valid ppl  5615.14\n","-----------------------------------------------------------------------------------------\n","=========================================================================================\n","| End of training | test loss  8.46 | test ppl  4722.95\n","=========================================================================================\n"]}]},{"cell_type":"markdown","metadata":{"id":"gpv4N8wl4w8p"},"source":["### 4. í•™ìŠµí•œ ì–¸ì–´ ëª¨ë¸ë¡œ ë¬¸ì¥ ìƒì„±\n","\n","\n","*   í•™ìŠµì´ ì™„ë£Œëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ random í•œ ë‹¨ì–´ë¥¼ input ìœ¼ë¡œ ë„£ì–´ì¤€ í›„ ì •í•´ì§„ ê°œìˆ˜ì˜ ë‹¨ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n","*   ìƒì„±í•œ ë¬¸ì¥ì„ decode í•˜ì—¬ (ì¦‰, idx2word ë¥¼ ì´ìš©í•´ id ë¥¼ word ë¡œ ë³€í™˜í•˜ì—¬) generate.txt íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤.\n","\n"]},{"cell_type":"code","metadata":{"id":"GQjeaKjO4JAh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671194233098,"user_tz":-540,"elapsed":1661,"user":{"displayName":"ì´ë‹¤ì›","userId":"11923633709773630529"}},"outputId":"1e12ac1b-cdd9-4fc5-ad02-f02fd2b9a61a"},"source":["###############################################################################\n","# Language Modeling on Wikitext-2\n","#\n","# This file generates new sentences sampled from the language model\n","#\n","###############################################################################\n","\n","import torch\n","\n","# Model parameters.\n","test_args = easydict.EasyDict({\n","    \"data\"      : './data/wikitext-2',  # location of data corpus\n","    \"checkpoint\": './model.pt',         # model checkpoint to use\n","    \"outf\"      : 'generate.txt',       # output file for generated text\n","    \"words\"     : 1000,                 # number of words to generate\n","    \"seed\"      : 1111,                 # random seed\n","    \"cuda\"      : True,                 # use CUDA\n","    \"temperature\": 1.0,                 # temperature - higher will increase diversity\n","    \"log_interval\": 100                 # reporting interval\n","})\n","\n","# Set the random seed manually for reproducibility.\n","torch.manual_seed(test_args.seed)\n","if torch.cuda.is_available():\n","    if not test_args.cuda:\n","        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n","\n","device = torch.device(\"cuda\" if test_args.cuda else \"cpu\")\n","\n","if test_args.temperature < 1e-3:\n","    parser.error(\"--temperature has to be greater or equal 1e-3\")\n","\n","with open(test_args.checkpoint, 'rb') as f:\n","    model = torch.load(f).to(device)\n","model.eval()\n","\n","# corpus = Corpus(test_args.data)\n","# ntokens = len(subword_corpus.dictionary)\n","\n","hidden = model.init_hidden(1)\n","input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n","\n","with open(test_args.outf, 'w') as outf:\n","    with torch.no_grad():  # no tracking history\n","        for i in range(test_args.words):\n","            output, hidden = model(input, hidden)\n","            word_weights = output.squeeze().div(test_args.temperature).exp().cpu()\n","            word_idx = torch.multinomial(word_weights, 1)[0]\n","            input.fill_(word_idx)\n","\n","            word = subword_corpus.dictionary.idx2word[word_idx]\n","\n","            outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n","\n","            if i % test_args.log_interval == 0:\n","                print('| Generated {}/{} words'.format(i, test_args.words))"],"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["| Generated 0/1000 words\n","| Generated 100/1000 words\n","| Generated 200/1000 words\n","| Generated 300/1000 words\n","| Generated 400/1000 words\n","| Generated 500/1000 words\n","| Generated 600/1000 words\n","| Generated 700/1000 words\n","| Generated 800/1000 words\n","| Generated 900/1000 words\n"]}]},{"cell_type":"code","source":["import time\n","a = 10\n","while a:\n","    a = a+1\n","    time.sleep(100)"],"metadata":{"id":"YETna_73VjhF","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1671196708317,"user_tz":-540,"elapsed":2475226,"user":{"displayName":"ì´ë‹¤ì›","userId":"11923633709773630529"}},"outputId":"7dc4be07-25ed-41fd-a37e-afd7c68bd40f"},"execution_count":31,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-d69c812fe78d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"dqLfZB-1wHHK"},"source":["## Reference\n","[Pytorch Language Model](https://github.com/pytorch/examples/tree/master/word_language_model)\n"]}]}