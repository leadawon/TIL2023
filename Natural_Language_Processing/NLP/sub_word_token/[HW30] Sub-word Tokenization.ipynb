{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"markdown","metadata":{"id":"uOnz8OxdbN3y"},"source":["# Tokenization\n"]},{"cell_type":"markdown","metadata":{"id":"TZb_LOZr7XF_"},"source":["### Data Preparation\n","\n","\n","*   데이터(train, dev, test)가 저장된 드라이브를 마운트하여 데이터를 불러올 수 있는 경로를 준비합니다.\n","\n"]},{"cell_type":"code","metadata":{"id":"wkCrAHWVmUck","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671194120908,"user_tz":-540,"elapsed":2862,"user":{"displayName":"이다원","userId":"11923633709773630529"}},"outputId":"2ef9bcd6-994d-4741-d5f3-a2d2967e5e30"},"source":["import os, sys\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","\n","# 현재 파일 경로에 맞게 설정해주세요.\n","os.chdir('/content/drive/MyDrive/WARNING_PRIVATE_FOLDER/TIL2023/Natural_Language_Processing/NLP/sub_word_token/')"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NSVysy6N4QhW","executionInfo":{"status":"ok","timestamp":1671194123472,"user_tz":-540,"elapsed":2569,"user":{"displayName":"이다원","userId":"11923633709773630529"}},"outputId":"7460be37-9cdc-431b-c41b-c02c3cbf1937"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"e9yYsxMfyZsi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671194123473,"user_tz":-540,"elapsed":15,"user":{"displayName":"이다원","userId":"11923633709773630529"}},"outputId":"81b59f77-02fb-4915-d7f2-ec59e1bafef4"},"source":["# 현재 경로 확인\n","!pwd"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/WARNING_PRIVATE_FOLDER/TIL2023/Natural_Language_Processing/NLP/sub_word_token\n","/content/drive/My Drive/WARNING_PRIVATE_FOLDER/TIL2023/Natural_Language_Processing/NLP/sub_word_token\n"]}]},{"cell_type":"code","metadata":{"id":"ceOZZ8UOU4or","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671194126482,"user_tz":-540,"elapsed":3017,"user":{"displayName":"이다원","userId":"11923633709773630529"}},"outputId":"6a2c16d4-e474-4181-d786-14278d427fcf"},"source":["# 라이브러리 설치\n","!pip install transformers"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n"]}]},{"cell_type":"markdown","metadata":{"id":"NSNON1TAbj2i"},"source":["### Introduction\n","\n","\n","* 본 과제의 목적은 Subword tokenization의 필요성을 직접 느껴보는 것입니다.\n","* Subword tokenization 기반 language model을 구현하면서 이전 과제의 Word-level language model과 비교해보는 시간을 갖겠습니다. 추가적으로 RNN을 LSTM으로 변경했을 때의 성능 차이에 대해 살펴보겠습니다.\n","*   Subword-level language model을 구현하고, 주어진 데이터를 가공하여 모델을 학습한 후 학습된 언어 모델을 이용해 문장을 생성합니다.\n"]},{"cell_type":"markdown","metadata":{"id":"_xzSMF9RJyzg"},"source":["\n","```\n","💡 Subword는 무엇인가요?\n","\n","Subword는 하나의 단어를 여러개의 단위로 분리했을 때 하나의 단위를 나타냅니다. \"subword\"를 subword 단위로 나타낸 하나의 예시는 다음과 같습니다.\n","\n","\"sub\" + \"word\"\n","\n","sub라는 접두사와 word라고 하는 어근으로 나누어 \"subword\"라고 하는 word를 2개의 subword로 나타냈습니다.\n","\n","이외에도 다양한 형태의 subword로 나타낼 수 있습니다. (e.g., \"su\" + \"bword\", \"s\" + \"ubword\", \"subwor\" + \"d\")\n","```"]},{"cell_type":"markdown","metadata":{"id":"zrgY-yRfVJON"},"source":["```\n","💡 tokenization은 무엇인가요?\n","\n","tokenization은 주어진 입력 데이터를 자연어처리 모델이 인식할 수 있는 단위로 변환해주는 방법입니다. \n","\n","💡 word tokenization은요?\n","\n","word tokenization의 경우 \"단어\"가 자연어처리 모델이 인식하는 단위가 됩니다.\n","\"I have a meal\"이라고 하는 문장을 가지고 word tokenization을 하면 다음과 같습니다. \n","\n","- ['I', 'have', 'a', 'meal']\n","\n","영어의 경우 대부분 space를 기준으로 단어가 정의되기 때문에 .split()을 이용해 쉽게 word tokenization을 구현할 수 있습니다.\n","영어에서 word tokenization은 space tokenization이라고도 할 수 있고, \n","subword tokenization 이전에 수행되는 pre-tokenization 방법으로도 많이 사용됩니다.\n","\n","\"나는 밥을 먹는다\"라는 문장을 word tokenization하면 다음과 같습니다.\n","- ['나', '는', '밥', '을', '먹는다']\n","\n","한국어에서 \"단어\"는 공백(space)을 기준으로 정의되지 않습니다. 이는 한국어가 갖고 있는 \"교착어\"로서의 특징 때문입니다. \n","체언 뒤에 조사가 붙는 것이 대표적인 특징이며 의미 단위가 구분되고 자립성이 있기 때문에 조사는 \"단어\"입니다.\n","\n","한국어에서는 pre-tokenization 방법으로 space tokenization을 사용하지 않고 형태소 분석기를 활용하고 있습니다.\n","```\n","\n","(참고1: [국립 국어원: \"조사는 단어이다\"](https://www.korean.go.kr/front/onlineQna/onlineQnaView.do?mn_id=216&qna_seq=26915#:~:text='%EC%A1%B0%EC%82%AC'%EB%8A%94%20%EC%99%84%EC%A0%84%ED%95%9C%20%EC%9E%90%EB%A6%BD%EC%84%B1%EC%9D%80,%ED%95%98%EC%97%AC%20%EB%8B%A8%EC%96%B4%EB%A1%9C%20%EC%B2%98%EB%A6%AC%ED%95%A9%EB%8B%88%EB%8B%A4.) )\n","\n","(참고2: [Huggingface: Pre-tokenization](https://huggingface.co/docs/tokenizers/python/latest/pipeline.html#pre-tokenization))\n","\n","(참고3: [Konlpy: 형태소 분석기](https://konlpy-ko.readthedocs.io/ko/v0.4.3/morph/))"]},{"cell_type":"markdown","metadata":{"id":"BVQgjQJdAbWh"},"source":["```\n","💡 그럼 Subword tokenization은 무엇인가요?\n","\n","Subword tokenizaiton은 말 그대로 subword 단위로 tokenization을 한다는 뜻입니다.\n","방금 전 word tokenization을 수행했던 문장을 이용해 subword tokenization을 수행한 예시를 보겠습니다.\n","\n","Subword tokenization을 적용했을 때는 다음과 같이 tokenization이 될 수 있습니다.\n","\n","Example 1\n","\n","\"I have a meal\" -> ['I', 'hav', 'e', 'a', 'me', 'al']\n","\"나는 밥을 먹는다\" -> ['나', '는', '밥', '을', '먹는', '다']\n","\n","word 단위가 아니라 그보다 더 잘게 쪼갠 subword 단위로 문장을 tokenization합니다.\n","\n","위에서 말씀드린 것과 같이 여러가지 경우의 수가 가능합니다.\n","\n","Example 2\n","\n","\"I have a meal\" -> ['I', 'ha', 've', 'a', 'mea', 'l']\n","\"나는 밥을 먹는다\" -> ['나', '는', '밥', '을', '먹', '는다']\n","\n","그렇지만 기본적으로 공백을 넘어선 subword를 구성하진 않습니다.\n","예를 들어 다음과 같이 tokenizaiton을 수행하진 않습니다.\n","\n","Example 3\n","\n","\"I have a meal\" -> ['Iha', 've', 'am', 'ea', 'l']\n","\"나는 밥을 먹는다\" -> ['나는밥', '을먹', '는다']\n","```\n","\n","(참고4: [Huggingface: subword-tokenization](https://huggingface.co/transformers/tokenizer_summary.html#subword-tokenization))"]},{"cell_type":"markdown","metadata":{"id":"SPRNaFhMEK67"},"source":["```\n","💡 Subword tokenization은 왜 필요한가요?\n","\n","word tokenization 코드를 불러와 그 필요성을 생각해 봅시다.\n","\n","```"]},{"cell_type":"code","metadata":{"id":"DESsQzhwGVST","executionInfo":{"status":"ok","timestamp":1671194126483,"user_tz":-540,"elapsed":18,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"source":["import os\n","from io import open\n","import torch"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"SWd09aUBGWa9","executionInfo":{"status":"ok","timestamp":1671194126484,"user_tz":-540,"elapsed":16,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"source":["class Dictionary(object):\n","    def __init__(self):\n","        self.word2idx = {'<unk>': 0}\n","        self.idx2word = ['<unk>']\n","\n","    def add_word(self, word):\n","        if word not in self.word2idx:\n","            self.idx2word.append(word)\n","            self.word2idx[word] = len(self.idx2word) - 1\n","        return self.word2idx[word]\n","\n","    def __len__(self):\n","        return len(self.idx2word)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"xQIyGQlfd9Os","executionInfo":{"status":"ok","timestamp":1671194126484,"user_tz":-540,"elapsed":14,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"source":["class Corpus(object):\n","    def __init__(self, path):\n","        self.dictionary = Dictionary()\n","        \"\"\"Tokenizes a text file.\"\"\"\n","        assert os.path.exists(path)\n","        # Add words to the dictionary\n","        with open(os.path.join(path, 'train.txt'), 'r', encoding=\"utf8\") as f:\n","            for line in f:\n","                words = line.split() + ['<eos>']\n","                for word in words:\n","                    self.dictionary.add_word(word)\n","\n","        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n","        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n","        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n","\n","    def tokenize(self, path):\n","        # Tokenize file content\n","        with open(path, 'r', encoding=\"utf8\") as f:\n","            idss = []\n","            for line in f:\n","                words = line.split() + ['<eos>']\n","                ids = []\n","                for word in words:\n","                    try:\n","                        ids.append(self.dictionary.word2idx[word])\n","                    except:\n","                        print(word)\n","                        ids.append(0)\n","                idss.append(torch.tensor(ids).type(torch.int64))\n","            ids = torch.cat(idss)\n","\n","        return ids"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"a55D0z53eLzB","executionInfo":{"status":"ok","timestamp":1671194126484,"user_tz":-540,"elapsed":13,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"source":["import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class RNNModel(nn.Module):\n","    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n","\n","    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5):\n","        super(RNNModel, self).__init__()\n","        self.ntoken = ntoken\n","        self.drop = nn.Dropout(dropout)\n","        self.encoder = nn.Embedding(ntoken, ninp)\n","        if rnn_type in ['LSTM', 'GRU']:\n","            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout) #nn.rnn_type(...)\n","        else:\n","            try:\n","                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n","            except KeyError:\n","                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n","                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n","            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n","        self.decoder = nn.Linear(nhid, ntoken)\n","\n","        self.init_weights()\n","\n","        self.rnn_type = rnn_type\n","        self.nhid = nhid\n","        self.nlayers = nlayers\n","\n","    def init_weights(self):\n","        initrange = 0.1\n","        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n","        nn.init.zeros_(self.decoder.weight)\n","        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n","\n","    def forward(self, input, hidden):\n","        emb = self.drop(self.encoder(input))\n","        output, hidden = self.rnn(emb, hidden)\n","        output = self.drop(output)\n","        decoded = self.decoder(output)\n","        decoded = decoded.view(-1, self.ntoken)\n","        return F.log_softmax(decoded, dim=1), hidden\n","\n","    def init_hidden(self, bsz):\n","        weight = next(self.parameters())\n","        if self.rnn_type == 'LSTM':\n","            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n","                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n","        else:\n","            return weight.new_zeros(self.nlayers, bsz, self.nhid)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"dDQTvkdJdZOk","executionInfo":{"status":"ok","timestamp":1671194126485,"user_tz":-540,"elapsed":13,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"source":["import time\n","import math\n","import os\n","import torch\n","import torch.nn as nn\n","\n","import easydict\n","args = easydict.EasyDict({\n","    \"data\"    : './data/wikitext-2',    # location of the data corpus\n","    \"model\"   : 'RNN_TANH',             # type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU)\n","    \"emsize\"  : 200,                    # size of word embeddings\n","    \"nhid\"    : 512,                    # number of hidden units per layer\n","    \"nlayers\" : 2,                      # number of layers\n","    \"lr\"      : 20,                     # initial learning rate\n","    \"clip\"    : 0.25,                   # gradient clipping\n","    \"epochs\"  : 6,                      # upper epoch limit\n","    \"batch_size\": 20,                   # batch size\n","    \"bptt\"    : 35,                     # sequence length\n","    \"dropout\" : 0.2,                    # dropout applied to layers (0 = no dropout)\n","    \"seed\"    : 1111,                   # random seed\n","    \"cuda\"    : True,                   # use CUDA\n","    \"log_interval\": 200,                # report interval\n","    \"save\"    : 'model.pt',             # path to save the final model\n","    \"dry_run\" : True,                   # verify the code and the model\n","\n","})\n","\n","# 디바이스 설정\n","device = torch.device(\"cuda\" if args.cuda else \"cpu\")"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xnsZkQqUHb6Z"},"source":["```\n","train.txt의 문장들을 word tokenization 해보고 단어들의 개수를 세어보겠습니다\n","```"]},{"cell_type":"code","metadata":{"id":"2aSB9Hk4HjyO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671194128353,"user_tz":-540,"elapsed":1880,"user":{"displayName":"이다원","userId":"11923633709773630529"}},"outputId":"d8db68e3-f33e-47ea-d5c7-c4ce3407067f"},"source":["corpus = Corpus('./data/wikitext-2')\n","ntokens = len(corpus.dictionary)\n","print(ntokens)"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["33278\n"]}]},{"cell_type":"markdown","metadata":{"id":"OBkvRpEcKEvd"},"source":["```\n","이전 과제에 사용된 embedding dimension의 크기는 200이므로 word embedding에 사용된 parameter의 수는 33278 x 200 (6,655,600개)입니다.\n","\n","그렇다면, RNN 모델에 사용되는 weight의 parameter 개수는 몇개인지 간단한 함수를 이용해 확인해보겠습니다\n","```"]},{"cell_type":"code","metadata":{"id":"rGjG2j-fKbJu","executionInfo":{"status":"ok","timestamp":1671194128354,"user_tz":-540,"elapsed":13,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"source":["model = RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout)"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"yTiKccVXLrvx","executionInfo":{"status":"ok","timestamp":1671194128355,"user_tz":-540,"elapsed":12,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"FvRPOxLCLByf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671194128356,"user_tz":-540,"elapsed":12,"user":{"displayName":"이다원","userId":"11923633709773630529"}},"outputId":"ecd4bcd0-456c-4839-f996-a53dac0b940a"},"source":["print(f\"Word embedding parameter 개수: {count_parameters(model.encoder)}\")\n","print(f\"RNN parameter 개수: {count_parameters(model.rnn)}\")"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Word embedding parameter 개수: 6655600\n","RNN parameter 개수: 890880\n"]}]},{"cell_type":"markdown","metadata":{"id":"nlUFvgTNM1Od"},"source":["```\n","💡 RNN parameter, Word embedding parameter 개수를 비교해보면 word embedding parameter의 개수가 RNN 모델의 parameter보다 압도적으로 많습니다.\n","\n","word embedding을 사용하는 경우 training에 사용되는 text file의 크기가 커질수록 word embedding parameter는 \n","더 커지게 되고 전체 parameter 대비 word embedding이 차지하는 비중은 매우 높아집니다.\n","```"]},{"cell_type":"markdown","metadata":{"id":"o7WfyYBrPpca"},"source":["```\n","✨ 이런 parameter 비중의 비대칭성을 해결하기 위해 처음에는 character-level tokenization 방법이 주목을 받았습니다. \n","말 그대로 하나의 글자를 기준으로 tokenization을 하는건데요.\n","이전 예시를 character 기반 tokenization을 하면 다음과 같습니다.\n","\n","\"I have a meal\" -> ['I', 'h', 'a', 'v', 'e', 'a', 'm', 'e', 'a', 'l']\n","\"나는 밥을 먹는다\" -> ['나', '는', '밥', '을', '먹', '는', '다']\n","\n","그러나, character 기반 tokenization 역시 지나치게 긴 sequence 길이, 성능 저하 등의 문제를 겪으며 \n","subword tokenization이 각광을 받게 되었습니다.\n","\n","❓ word tokenization 기법과 구분되는 subword tokenization의 장점을 하나만 더 생각해볼까요?\n","\n","subword tokenization의 장점은 Out-of-vocabulary (OOV) 문제에서 상대적으로 자유롭다는 것입니다.\n","\n","일반적으로 subword들은 최소 철자 단위에서 하나씩 더 긴 subword를 추가하는 방식으로 만들어집니다.\n","\n","예를 들어, 영어의 경우 a~z의 알파벳부터 시작해서 두글자, 세글자, 네글자 subword 등으로 확장해나가며 \n","subword를 추가해 단어를 구성하고 이를 바탕으로 subword tokenization을 수행하기 때문에 다른 언어를 tokenization하지 않는다면 \n","OOV 문제에서 자유롭다고 볼 수 있습니다.\n","\n","대표적인 subword tokenization에 사용되는 algorithm 중 하나인 byte pair encoding은 선택 과제 3에서 살펴볼 수 있으니 참고해주세요 !\n","\n","✨ 그럼 이제부터 BERT 모델에서 사용한 subword tokenization algorithm을 이용해 language modeling task를 수행해보겠습니다. \n","subword tokenizer는 transformers 라이브러리를 이용해 쉽게 불러올 수 있습니다.\n","```\n","\n","(참고5: [Huggingface: subword tokenization](https://huggingface.co/transformers/tokenizer_summary.html#subword-tokenization))\n"]},{"cell_type":"code","metadata":{"id":"WZKyn3PKUuBg","executionInfo":{"status":"ok","timestamp":1671194132877,"user_tz":-540,"elapsed":4531,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"rwubp25xVUt2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671194132878,"user_tz":-540,"elapsed":29,"user":{"displayName":"이다원","userId":"11923633709773630529"}},"outputId":"4c4d9713-0664-4dfd-f676-37ac4249c811"},"source":["# subword tokenization 예시\n","print(tokenizer.tokenize('Natural language expert training course'))\n","print(tokenizer.tokenize('Goorm X KAIST'))"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["['Natural', 'language', 'expert', 'training', 'course']\n","['Go', '##orm', 'X', 'K', '##A', '##IS', '##T']\n"]}]},{"cell_type":"code","metadata":{"id":"vLfVolP3UKos","executionInfo":{"status":"ok","timestamp":1671194132878,"user_tz":-540,"elapsed":25,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"source":["class Corpus(object):\n","    def __init__(self, path):\n","        self.dictionary = Dictionary()\n","        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n","        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n","        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n","\n","    def tokenize(self, path):\n","        assert os.path.exists(path)\n","        # Add words to the dictionary\n","        with open(path, 'r', encoding=\"utf8\") as f:\n","            for line in f:\n","                words = tokenizer.tokenize(line.strip()) + ['<eos>']\n","                for word in words:\n","                    self.dictionary.add_word(word)\n","\n","        # Tokenize file content\n","        with open(path, 'r', encoding=\"utf8\") as f:\n","            idss = []\n","            for line in f:\n","                words = tokenizer.tokenize(line.strip()) + ['<eos>']\n","                ids = []\n","                for word in words:\n","                    ids.append(self.dictionary.word2idx[word])\n","                idss.append(torch.tensor(ids).type(torch.int64))\n","            ids = torch.cat(idss)\n","\n","        return ids"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BvtarB2DYiLb"},"source":["```\n","그러면 이제 다시 모델을 선언하고 parameter의 개수를 살펴보겠습니다.\n","```"]},{"cell_type":"code","metadata":{"id":"wLKqis0hY5Or","executionInfo":{"status":"ok","timestamp":1671194205157,"user_tz":-540,"elapsed":72303,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"source":["subword_corpus = Corpus('./data/wikitext-2')\n","ntokens = len(subword_corpus.dictionary)\n","subwordmodel = RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout)"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"_RB4DQ-QZCBd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671194205158,"user_tz":-540,"elapsed":18,"user":{"displayName":"이다원","userId":"11923633709773630529"}},"outputId":"f2fba98c-47ee-4f3b-af6a-d4cd5dab52c3"},"source":["print(f\"Word embedding parameter 개수: {count_parameters(subwordmodel.encoder)}\")\n","print(f\"RNN parameter 개수: {count_parameters(subwordmodel.rnn)}\")"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Word embedding parameter 개수: 4619000\n","RNN parameter 개수: 890880\n"]}]},{"cell_type":"markdown","metadata":{"id":"q_5y4M6k1HR6"},"source":["```\n","이전에 비해 embedding parameter 개수는 확연히 줄어들었습니다.\n","6,655,600개 -> 4,619,000개\n","\n","그러면 이제 subword 기반의 언어 모델 성능을 살펴보겠습니다.\n","```"]},{"cell_type":"code","metadata":{"id":"JlSdSCKvd23M","executionInfo":{"status":"ok","timestamp":1671194207712,"user_tz":-540,"elapsed":2570,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"source":["###############################################################################\n","# Load data\n","###############################################################################\n","\n","# Starting from sequential data, batchify arranges the dataset into columns.\n","# For instance, with the alphabet as the sequence and batch size 4, we'd get\n","# ┌ a g m s ┐\n","# │ b h n t │\n","# │ c i o u │\n","# │ d j p v │\n","# │ e k q w │\n","# └ f l r x ┘.\n","# These columns are treated as independent by the model, which means that the\n","# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n","# batch processing.\n","\n","def batchify(data, bsz):\n","    # Work out how cleanly we can divide the dataset into bsz parts.\n","    nbatch = data.size(0) // bsz\n","    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n","    data = data.narrow(0, 0, nbatch * bsz)\n","    # Evenly divide the data across the bsz batches.\n","    data = data.view(bsz, -1).t().contiguous()\n","    return data.to(device)\n","\n","eval_batch_size = 10\n","train_data = batchify(subword_corpus.train, args.batch_size)\n","val_data = batchify(subword_corpus.valid, eval_batch_size)\n","test_data = batchify(subword_corpus.test, eval_batch_size)"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"R7iRv2rHe959","executionInfo":{"status":"ok","timestamp":1671194208899,"user_tz":-540,"elapsed":1237,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"source":["###############################################################################\n","# Build the model\n","###############################################################################\n","\n","model = RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout).to(device)\n","criterion = nn.NLLLoss()"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"HiLaw_Bte_nJ","executionInfo":{"status":"ok","timestamp":1671194208902,"user_tz":-540,"elapsed":15,"user":{"displayName":"이다원","userId":"11923633709773630529"}}},"source":["###############################################################################\n","# Training code1 - define functions\n","###############################################################################\n","\n","def repackage_hidden(h):\n","    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n","\n","    if isinstance(h, torch.Tensor):\n","        return h.detach()\n","    else:\n","        return tuple(repackage_hidden(v) for v in h)\n","\n","\n","# get_batch subdivides the source data into chunks of length args.bptt.\n","# If source is equal to the example output of the batchify function, with\n","# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n","# ┌ a g m s ┐ ┌ b h n t ┐\n","# └ b h n t ┘ └ c i o u ┘\n","# Note that despite the name of the function, the subdivison of data is not\n","# done along the batch dimension (i.e. dimension 1), since that was handled\n","# by the batchify function. The chunks are along dimension 0, corresponding\n","# to the seq_len dimension in the LSTM.\n","\n","def get_batch(source, i):\n","    seq_len = min(args.bptt, len(source) - 1 - i)\n","    data = source[i:i+seq_len]\n","    target = source[i+1:i+1+seq_len].view(-1)\n","    return data, target\n","\n","\n","def evaluate(data_source):\n","    # Turn on evaluation mode which disables dropout.\n","    model.eval()\n","    total_loss = 0.\n","    ntokens = len(subword_corpus.dictionary)\n","    hidden = model.init_hidden(eval_batch_size)\n","    with torch.no_grad():\n","        for i in range(0, data_source.size(0) - 1, args.bptt):\n","            data, targets = get_batch(data_source, i)\n","            output, hidden = model(data, hidden)\n","            hidden = repackage_hidden(hidden)\n","            total_loss += len(data) * criterion(output, targets).item()\n","    return total_loss / (len(data_source) - 1)\n","\n","\n","def train():\n","    # Turn on training mode which enables dropout.\n","    model.train()\n","    total_loss = 0.\n","    start_time = time.time()\n","    ntokens = len(subword_corpus.dictionary)\n","    hidden = model.init_hidden(args.batch_size)\n","    for batch, i in enumerate(range(0, train_data.size(0) - 1, args.bptt)):\n","        data, targets = get_batch(train_data, i)\n","        # Starting each batch, we detach the hidden state from how it was previously produced.\n","        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n","        model.zero_grad()\n","\n","        hidden = repackage_hidden(hidden)\n","        output, hidden = model(data, hidden)\n","\n","        loss = criterion(output, targets)\n","        loss.backward()\n","\n","        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n","        for p in model.parameters():\n","            p.data.add_(p.grad, alpha=-lr)\n","\n","        total_loss += loss.item()\n","\n","        if batch % args.log_interval == 0 and batch > 0:\n","            cur_loss = total_loss / args.log_interval\n","            elapsed = time.time() - start_time\n","            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n","                    'loss {:5.2f} | ppl {:8.2f}'.format(\n","                epoch, batch, len(train_data) // args.bptt, lr,\n","                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n","            total_loss = 0\n","            start_time = time.time()\n","        if args.dry_run:\n","            break"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"A6m-cdbm7PvS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671194231452,"user_tz":-540,"elapsed":22562,"user":{"displayName":"이다원","userId":"11923633709773630529"}},"outputId":"2fd7a593-2a06-473d-89be-d60e5c6a09af"},"source":["###############################################################################\n","# Training code2 - run \n","###############################################################################\n","\n","# Loop over epochs.\n","lr = args.lr\n","best_val_loss = None\n","\n","# At any point you can hit Ctrl + C to break out of training early.\n","try:\n","    for epoch in range(1, args.epochs+1):\n","        epoch_start_time = time.time()\n","        train()\n","        val_loss = evaluate(val_data)\n","        print('-' * 89)\n","        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n","                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n","                                           val_loss, math.exp(val_loss)))\n","        print('-' * 89)\n","        # Save the model if the validation loss is the best we've seen so far.\n","        if not best_val_loss or val_loss < best_val_loss:\n","            with open(args.save, 'wb') as f:\n","                torch.save(model, f)\n","            best_val_loss = val_loss\n","        else:\n","            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n","            lr /= 4.0\n","except KeyboardInterrupt:\n","    print('-' * 89)\n","    print('Exiting from training early')\n","\n","# Load the best saved model.\n","with open(args.save, 'rb') as f:\n","    model = torch.load(f)\n","    # after load the rnn params are not a continuous chunk of memory\n","    # this makes them a continuous chunk, and will speed up forward pass\n","    # Currently, only rnn model supports flatten_parameters function.\n","    if args.model in ['RNN_TANH', 'RNN_RELU', 'LSTM', 'GRU']:\n","        model.rnn.flatten_parameters()\n","\n","# Run on test data.\n","test_loss = evaluate(test_data)\n","print('=' * 89)\n","print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n","    test_loss, math.exp(test_loss)))\n","print('=' * 89)"],"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["-----------------------------------------------------------------------------------------\n","| end of epoch   1 | time:  3.32s | valid loss  8.98 | valid ppl  7955.79\n","-----------------------------------------------------------------------------------------\n","-----------------------------------------------------------------------------------------\n","| end of epoch   2 | time:  2.97s | valid loss 12.64 | valid ppl 308226.21\n","-----------------------------------------------------------------------------------------\n","-----------------------------------------------------------------------------------------\n","| end of epoch   3 | time:  3.00s | valid loss 10.88 | valid ppl 52931.24\n","-----------------------------------------------------------------------------------------\n","-----------------------------------------------------------------------------------------\n","| end of epoch   4 | time:  3.00s | valid loss  9.37 | valid ppl 11789.07\n","-----------------------------------------------------------------------------------------\n","-----------------------------------------------------------------------------------------\n","| end of epoch   5 | time:  3.06s | valid loss  8.95 | valid ppl  7672.45\n","-----------------------------------------------------------------------------------------\n","-----------------------------------------------------------------------------------------\n","| end of epoch   6 | time:  3.04s | valid loss  8.63 | valid ppl  5615.14\n","-----------------------------------------------------------------------------------------\n","=========================================================================================\n","| End of training | test loss  8.46 | test ppl  4722.95\n","=========================================================================================\n"]}]},{"cell_type":"markdown","metadata":{"id":"gpv4N8wl4w8p"},"source":["### 4. 학습한 언어 모델로 문장 생성\n","\n","\n","*   학습이 완료된 모델을 불러와 random 한 단어를 input 으로 넣어준 후 정해진 개수의 단어를 생성합니다.\n","*   생성한 문장을 decode 하여 (즉, idx2word 를 이용해 id 를 word 로 변환하여) generate.txt 파일에 저장합니다.\n","\n"]},{"cell_type":"code","metadata":{"id":"GQjeaKjO4JAh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671194233098,"user_tz":-540,"elapsed":1661,"user":{"displayName":"이다원","userId":"11923633709773630529"}},"outputId":"1e12ac1b-cdd9-4fc5-ad02-f02fd2b9a61a"},"source":["###############################################################################\n","# Language Modeling on Wikitext-2\n","#\n","# This file generates new sentences sampled from the language model\n","#\n","###############################################################################\n","\n","import torch\n","\n","# Model parameters.\n","test_args = easydict.EasyDict({\n","    \"data\"      : './data/wikitext-2',  # location of data corpus\n","    \"checkpoint\": './model.pt',         # model checkpoint to use\n","    \"outf\"      : 'generate.txt',       # output file for generated text\n","    \"words\"     : 1000,                 # number of words to generate\n","    \"seed\"      : 1111,                 # random seed\n","    \"cuda\"      : True,                 # use CUDA\n","    \"temperature\": 1.0,                 # temperature - higher will increase diversity\n","    \"log_interval\": 100                 # reporting interval\n","})\n","\n","# Set the random seed manually for reproducibility.\n","torch.manual_seed(test_args.seed)\n","if torch.cuda.is_available():\n","    if not test_args.cuda:\n","        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n","\n","device = torch.device(\"cuda\" if test_args.cuda else \"cpu\")\n","\n","if test_args.temperature < 1e-3:\n","    parser.error(\"--temperature has to be greater or equal 1e-3\")\n","\n","with open(test_args.checkpoint, 'rb') as f:\n","    model = torch.load(f).to(device)\n","model.eval()\n","\n","# corpus = Corpus(test_args.data)\n","# ntokens = len(subword_corpus.dictionary)\n","\n","hidden = model.init_hidden(1)\n","input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n","\n","with open(test_args.outf, 'w') as outf:\n","    with torch.no_grad():  # no tracking history\n","        for i in range(test_args.words):\n","            output, hidden = model(input, hidden)\n","            word_weights = output.squeeze().div(test_args.temperature).exp().cpu()\n","            word_idx = torch.multinomial(word_weights, 1)[0]\n","            input.fill_(word_idx)\n","\n","            word = subword_corpus.dictionary.idx2word[word_idx]\n","\n","            outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n","\n","            if i % test_args.log_interval == 0:\n","                print('| Generated {}/{} words'.format(i, test_args.words))"],"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["| Generated 0/1000 words\n","| Generated 100/1000 words\n","| Generated 200/1000 words\n","| Generated 300/1000 words\n","| Generated 400/1000 words\n","| Generated 500/1000 words\n","| Generated 600/1000 words\n","| Generated 700/1000 words\n","| Generated 800/1000 words\n","| Generated 900/1000 words\n"]}]},{"cell_type":"code","source":["import time\n","a = 10\n","while a:\n","    a = a+1\n","    time.sleep(100)"],"metadata":{"id":"YETna_73VjhF","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1671196708317,"user_tz":-540,"elapsed":2475226,"user":{"displayName":"이다원","userId":"11923633709773630529"}},"outputId":"7dc4be07-25ed-41fd-a37e-afd7c68bd40f"},"execution_count":31,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-d69c812fe78d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"dqLfZB-1wHHK"},"source":["## Reference\n","[Pytorch Language Model](https://github.com/pytorch/examples/tree/master/word_language_model)\n"]}]}