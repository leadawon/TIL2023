{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1xaNzdWI2m8gXhThAmv_rB8CoZyEn93CU","timestamp":1670204620473}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZhAEjARBRPGb"},"source":["# NLP Quiz1\n","\n","> 본 퀴즈에서는 강의에서 직접적으로 다루지 않은 문제들도 포함되어 있습니다. 성취도 확인용으로 진행되는 시험이니 한 주 동안 배운 내용을 복습하고 새로운 문제들을 공부하는 데 의미를 두시면 좋을 것 같습니다."]},{"cell_type":"markdown","metadata":{"id":"sSqNrv0c52kO"},"source":["### Part 1. 다음 명제에 대해, True/False를 판단하시오. 판단 근거를 간략하게 설명하시오."]},{"cell_type":"markdown","metadata":{"id":"2Z081ktX4Ujo"},"source":["Bag-of-Words 가정을 기반으로한 워드 임베딩은 문장 내 단어의 등장 순서를 고려한다."]},{"cell_type":"markdown","metadata":{"id":"-RJeOeKRRfSs"},"source":["A:\n","False. 빈도만 고려한다.\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XFl5wV-5daU9"},"source":["Bag-of-Words 표현형을 만들 때, 유사한 의미를 가진 단어들은 벡터 공간 상에서 다른 단어 보다 더 가까이 위치해 있다."]},{"cell_type":"markdown","metadata":{"id":"zR1O9-Ued58u"},"source":["A:\n","False. 단어는 one-hot-vertor로 나타내서 서로 내적이 0이다. word2vec같이 단어와 단어의 관계를 나타내지는 않는다.\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"L3feYy_f6AO7"},"source":["Navie Bayes 기반 topic classification 모델에서는 문장 내 단어의 등장 확률은 서로 독립이다. "]},{"cell_type":"markdown","metadata":{"id":"7mblo4wgRnrv"},"source":["A:\n","True. 맞다. 전제가 그렇다.\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"s05HjcfO78q7"},"source":["Topic Modeling 에서 문서 내 단어들은 빈도 기반의 벡터로 표현된다."]},{"cell_type":"markdown","metadata":{"id":"hiTF_L-LRo8c"},"source":["A:\n","True. term이 document에서 나온 횟수로 나타낸다.\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MLhrKOQT8xN7"},"source":["Word2Vec 학습시 각 단어는 입력 문장에 등장하는 단어 모두를 고려하여 word representation을 업데이트한다."]},{"cell_type":"markdown","metadata":{"id":"eeB78enNRx6S"},"source":["A:\n","False. window size를 통해서 인접한 단어를 다룬다.\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"A4BORmzMBNWU"},"source":["GloVe 학습시 각 단어는 문장 내 단어의 등장 여부 뿐만 아니라 학습 말뭉치 전체에서 동시에 등장하는 단어를 모두 고려하여 word representation을 업데이트 한다."]},{"cell_type":"markdown","metadata":{"id":"2b7KujmiRzDL"},"source":["A:\n","True. very very hard 같은 경우도 생각한다. 이때 input과 output이 같아서 행렬의 대각선에 값이 추가될 것이다.\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vIZy6AYwF--D"},"source":["RNN 의 one-to-many setting 에서 time step 0 이후의 input은 입력되지 않는다."]},{"cell_type":"markdown","metadata":{"id":"tqgxLBoJRzo7"},"source":["A:\n","True. image caption같이 한번 input이 들어오면 연속적으로 자막만 달고 입력은 없다.\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MwU7Z9Gq8gJN"},"source":["Gradient clipping 은 vanishing gradient problem 을 방지하는 방법이다."]},{"cell_type":"markdown","metadata":{"id":"Ge1HG71MR1Se"},"source":["A:\n","False gradient exploding을 방지한다.\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uzL4Ae2CERuI"},"source":["Bi-directional RNN/LSTM 모델은 language modeling task 학습이 가능하다."]},{"cell_type":"markdown","metadata":{"id":"Gn0brPMDR2OV"},"source":["A:\n","True 뒤의 단어로 앞의 단어를 예측할 수 있다면 더 좋다.\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qXiOSUY_Gff4"},"source":["Skip-gram에서 window size=5 인 경우 문장의 첫 2개 단어와 가장 뒷 단어 2개를 input으로 한 학습은 할 수 없다."]},{"cell_type":"markdown","metadata":{"id":"u1s-0No0R39R"},"source":["A:\n","True 타겟단어가 있을때 윈도우 5라는 것은앞과 뒤의 두개의단어를 본다는 의미이다. 세번째 단어부터 센다. 뒤에서 세번째 되어야 윈도우 사이즈 충족해서 스킵그램을 할 수 있다. 명제는 참이다.\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PmOd7T1LTV0I"},"source":["Word2vec 워드 임베딩은 두 단어가 정 반대의 의미를 가지고 있으면 두 단어의 임베딩 벡터는 서로 반대 방향을 가리킨다."]},{"cell_type":"markdown","metadata":{"id":"rckCoja8UuoS"},"source":["A:\n","false. 관련성이 높아야 벡터공간에서 비슷하다. 의미관계가 아니다. 관련성이다. 킹과 퀸이 비슷한 벡터공간상에 있다. 헐... 관련성이 높다.\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1BbgO9vkVJtr"},"source":["단어 $w$ 가 전체 문서에 많이 등장하면 TF-IDF($w$) 값이 커진다."]},{"cell_type":"markdown","metadata":{"id":"BGXQCk4eWbvU"},"source":["A:\n","False 많은 문서에서 등장하면 중요도가 떨어진다. DF값이 커지면 DF가 TF * log(N/DF) 이렇게 분모에 있기에 작아진다.\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nG6i1pj4gtKP"},"source":["### Part 2. 객관식/주관식 문항"]},{"cell_type":"markdown","metadata":{"id":"h6dnCH-yhhN0"},"source":["기계 번역 태스크를 수행하기 위해서 활용하기에 적절한 RNN 모델 구조를 고르시오. (객관식)\n","1. one-to-many\n","2. many-to-one\n","3. many-to-many"]},{"cell_type":"markdown","metadata":{"id":"RBwnOnlC1WF4"},"source":["A:\n","3. many to many\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6dL5bQ0_2CXv"},"source":["LSTM cell 에서 사용되는 activation function 의 종류를 모두 고르시오. (객관식)\n","1. sigmoid\n","2. relu\n","3. tanh\n","4. gelu"]},{"cell_type":"markdown","metadata":{"id":"WegUUjiS2NCD"},"source":["A:\n","1. sigmoid 3. tanh\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Tf0Dszgy1ZsB"},"source":["Word2Vec 의 두 가지 기법인 CBOW 와 Skip-gram 방법을 비교하시오. (주관식)"]},{"cell_type":"markdown","metadata":{"id":"MnMh1ip41qaj"},"source":["A:\n","CBOW는 window 안 주변 단어들로 중간의 단어를 예측하는 방법, Skip-gram은 중간에 있는 단어로 주변 단어를 예측한다.\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"X2x6J8wW1rS7"},"source":["LSTM 의 각 gate 의 역할을 간단히 설명하시오. (주관식)"]},{"cell_type":"markdown","metadata":{"id":"xpSBFen22Tcz"},"source":["A:\n","값을 흐리게 만든다. sigmoid는 0과 1사이의 값이고 이것을 곱하기 때문이다.\n","forget : cell state를 얼마나 잊느냐.\n","input : gate gate 를 얼마나 전달\n","gate : ht-1 ,xt 값\n","output : cellstate를 t+1에 얼마나 전달 할 것이냐.\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1yXsoFMHgJrQ"},"source":["LSTM 에서 cell state 의 역할 및 장점을 간단히 설명하시오. (주관식)"]},{"cell_type":"markdown","metadata":{"id":"V4aKbNiF8nra"},"source":["A:\n","hidden state는 이전 cell에서의 계산값이 주가 되지만 cell state는 그동안의 cell state의 값을 어느정도 가지고 있다. 덧셈에 기반해서 update하기 때문에 이전 셀의 정보를 어느정도 유지한다.\n","\n","gate highway 덧셈기반. 정보손실 안되서 short term memory문제를 완화.\n","타임시퀀스가 길어져도 타임스텝마다 gradient가 흐른다.\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"mvk53ZNJgq1D"},"source":[],"execution_count":null,"outputs":[]}]}