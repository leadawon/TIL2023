{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"markdown","metadata":{"id":"btDmoiRCRfMp"},"source":["# **[HW24] NaiveBayes Classifier**\n","1. Requirements\n","2. Data Preprocessing\n","3. Model Training\n","4. Evaluation"]},{"cell_type":"markdown","metadata":{"id":"3a3E1pbkSYSF"},"source":["## 1. Requirements"]},{"cell_type":"markdown","metadata":{"id":"4THGPr9QIjQY"},"source":["#### 1.1 필요한 패키지를 설치(install) 및 import 합니다."]},{"cell_type":"code","metadata":{"id":"6GKm6PwhiGxv","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5b3a4f21-ab8f-46f4-dab4-88d2e680d61d"},"source":["# 한국어 전처리 라이브러리 \n","!pip install konlpy"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.6.0)\n","Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.4.1)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.9.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (21.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->JPype1>=0.7.0->konlpy) (3.0.9)\n"]}]},{"cell_type":"code","metadata":{"id":"2srhF-lp4JxL"},"source":["from tqdm import tqdm\n","from collections import defaultdict\n","import math\n","\n","# POS(Part of Speech) tagger\n","from konlpy import tag  #품사태거"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wJ-s9c77IVUA"},"source":["#### 1.2 Train data 와 test data 를 준비합니다."]},{"cell_type":"code","metadata":{"id":"MCBnEQrfoL_C"},"source":["data = {}\n","# training data. input text 와 정답 label (긍정(1), 부정(0)) 로 구성.\n","\n","data['train'] = [{'text': \"정말 재미있습니다. 추천합니다.\"},\n","                {'text': \"기대했던 것보단 별로였네요.\"},\n","                {'text': \"지루해서 다시 보고 싶다는 생각이 안 드네요.\"},\n","                {'text': \"완전 최고입니다 ! 다시 보고 싶습니다.\"},\n","                {'text': \"연기도 연출도 다 만족스러웠습니다.\"},\n","                {'text': \"연기가 좀 별로였습니다.\"},\n","                {'text': \"연출도 좋았고 배우분들 연기도 최고입니다.\"},\n","                {'text': \"기념일에 방문했는데 연기도 연출도 다 좋았습니다.\"},\n","                {'text': \"전반적으로 지루했습니다. 저는 별로였네요.\"},\n","                {'text': \"CG에 조금 더 신경 썼으면 좋겠습니다.\"}\n","                ]\n","# test data\n","data['test'] = [{'text': \"최고입니다. 또 보고 싶네요.\"},\n","                {'text': \"별로였습니다. 되도록 보지 마세요.\"},\n","                {'text': \"다른 분들께 추천드릴 수 있을 만큼 연출도 연기도 만족했습니다.\"},\n","                {'text': \"연기가 좀 더 개선되었으면 좋겠습니다.\"}\n","                ]\n","\n","train_labels = [1, 0, 0, 1, 1, 0, 1, 1, 0, 0]\n","test_labels = [1, 0, 1, 0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rpgjbzqr6UR4"},"source":["### 2. Data Preprocessing\n"]},{"cell_type":"markdown","metadata":{"id":"wKwnFBZqMXm-"},"source":["#### 2.1 한글 형태소 분석기를 이용해서 주어진 데이터를 tokenize 합니다.\n","\n","오픈소스 형태소 분석기를 제공하는 파이썬 패키지 KoNLPy에서 제공하는 [꼬꼬마(Kkma) 형태소 분석기](https://konlpy.org/en/v0.5.2/api/konlpy.tag/#module-konlpy.tag._kkma)를 이용하여 tokenize 합니다."]},{"cell_type":"code","metadata":{"id":"bEzeYDmPjNLl"},"source":["# 형태소 분석기 선언\n","morph_analyzer = tag.Kkma() "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tftxirx_7sk7"},"source":["# tokenization 함수 정의\n","def tokenization(data, morph_analyzer):\n","    '''\n","    (input) data: list of data examples.\n","            morph_analyzer: morphological analyzer.\n","    (output) tokenized_data: list of tokenized data examples.\n","    '''\n","    tokenized_data = []\n","\n","    for example in tqdm(data):\n","        tokens = morph_analyzer.morphs(example['text']) #str 단위 받아서 morphs 메소드 실행. list안에 str형태로 형태소 분석.\n","        tokenized_data.append(tokens) #tokenized_data는 이차원 리스트로 data의 문장의 갯수의 행을 가지고 있음.\n","\n","    return tokenized_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I4VEZyFlCqi-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fbc65cbb-df75-4ea6-fa1f-2ac727f220eb"},"source":["# tokenization 함수를 이용한 데이터 tokenization\n","tokenized_data = {}\n","\n","tokenized_data['train'] = tokenization(data['train'], morph_analyzer)\n","tokenized_data['test'] = tokenization(data['test'], morph_analyzer)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:00<00:00, 72.34it/s]\n","100%|██████████| 4/4 [00:00<00:00, 38.51it/s]\n"]}]},{"cell_type":"code","metadata":{"id":"OEhn3uv2o2kt","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cef7d884-cbbf-47f2-dd9e-e799c189c109"},"source":["# tokenized_data 확인\n","tokenized_data['train']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['정말', '재미있', '습니다', '.', '추천', '하', 'ㅂ니다', '.'],\n"," ['기대', '하', '었', '더', 'ㄴ', '것', '보다', 'ㄴ', '별', '로', '이', '었', '네요', '.'],\n"," ['지루', '하', '어서', '다시', '보', '고', '싶', '다는', '생각', '이', '안', '들', '네요', '.'],\n"," ['완전', '최고', '이', 'ㅂ니다', '!', '다시', '보', '고', '싶', '습니다', '.'],\n"," ['연기', '도', '연출', '도', '다', '만족', '스럽', '었', '습니다', '.'],\n"," ['연기', '가', '좀', '별', '로', '이', '었', '습니다', '.'],\n"," ['연출', '도', '좋', '았', '고', '배우', '분', '들', '연기', '도', '최고', '이', 'ㅂ니다', '.'],\n"," ['기념일',\n","  '에',\n","  '방문',\n","  '하',\n","  '었',\n","  '는데',\n","  '연기',\n","  '도',\n","  '연출',\n","  '도',\n","  '다',\n","  '좋',\n","  '았',\n","  '습니다',\n","  '.'],\n"," ['전반적',\n","  '으로',\n","  '지루',\n","  '하',\n","  '었',\n","  '습니다',\n","  '.',\n","  '저',\n","  '는',\n","  '별',\n","  '로',\n","  '이',\n","  '었',\n","  '네요',\n","  '.'],\n"," ['CG', '에', '조금', '더', '신경', '쓰', '었', '으면', '좋', '겠', '습니다', '.']]"]},"metadata":{},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"olCz4j6VS0KJ"},"source":["#### 2.2 tokenization 결과를 이용해서 word to index dictionary 를 생성합니다.\n"]},{"cell_type":"code","metadata":{"id":"u2DXYDTTTChQ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f759f157-15df-4c95-e4f6-2f2ef65ff03b"},"source":["# train data의 tokenization 결과에서 unique token만 남긴 set으로 변환\n","tokens = [token for i in range(len(tokenized_data['train'])) for token in tokenized_data['train'][i] ] #형태소 분리 완료한 train data의 문장 하나씩 꺼내서 1차원 리스트에 concat\n","print(tokens)\n","\n","unique_train_tokens = set(tokens) #1차원 리스트에 train data의 유일한 형태소만 남기고 set형태로 변환.\n","\n","# NaiveBayes Classifier의 input에 들어갈 word의 index를 반환해주는 dictionary를 생성\n","word2index = defaultdict() # key: word, value: index of word\n","idx = 0\n","for token in tqdm(unique_train_tokens): #train data 유일토큰을 담는 set에서 유일토큰을 하나씩 꺼내서 인덱스를 매겨준다.\n","    word2index[token] = idx #형태소마다 각자 인덱스를 지니게 된다. 인덱스는 0부터 시작함.\n","    idx += 1"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['정말', '재미있', '습니다', '.', '추천', '하', 'ㅂ니다', '.', '기대', '하', '었', '더', 'ㄴ', '것', '보다', 'ㄴ', '별', '로', '이', '었', '네요', '.', '지루', '하', '어서', '다시', '보', '고', '싶', '다는', '생각', '이', '안', '들', '네요', '.', '완전', '최고', '이', 'ㅂ니다', '!', '다시', '보', '고', '싶', '습니다', '.', '연기', '도', '연출', '도', '다', '만족', '스럽', '었', '습니다', '.', '연기', '가', '좀', '별', '로', '이', '었', '습니다', '.', '연출', '도', '좋', '았', '고', '배우', '분', '들', '연기', '도', '최고', '이', 'ㅂ니다', '.', '기념일', '에', '방문', '하', '었', '는데', '연기', '도', '연출', '도', '다', '좋', '았', '습니다', '.', '전반적', '으로', '지루', '하', '었', '습니다', '.', '저', '는', '별', '로', '이', '었', '네요', '.', 'CG', '에', '조금', '더', '신경', '쓰', '었', '으면', '좋', '겠', '습니다', '.']\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 56/56 [00:00<00:00, 524288.00it/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"85oCOe0Xqcwk"},"source":["### 3. Model Training"]},{"cell_type":"markdown","metadata":{"id":"w3uuFi52qjh6"},"source":["#### 3.1 NaiveBayes Classifier 모델 클래스를 구현합니다.\n"]},{"cell_type":"code","metadata":{"id":"TsZlgjkBHAod"},"source":["class NaiveBayesClassifier():\n","    def __init__(self, word2index, k=0.1):\n","        \"\"\"\n","        (input) word2index: mapping a word to a pre-assigned index.\n","        \"\"\"\n","        self.k = k # for smoothing\n","        self.word2index = word2index\n","        self.priors = {} # Prior probability for each class, P(c) //// class : 0 or 1\n","        self.likelihoods = {} # Likelihood for each token, P(d|c)\n","\n","    def _set_priors(self, labels):\n","        \"\"\"\n","        Set prior probability for each class, P(c).\n","        Count the number of each class and calculate P(c) for each class.\n","        \"\"\"\n","        \n","        class_counts = defaultdict(int)\n","        ############################ ANSWER HERE ################################\n","        # TODO 1: Count the number of each class\n","        for label in tqdm(labels): #prior은 good bad 클래스의 확률이므로 라벨의 갯수로 구할 수 있다.\n","            class_counts[label] += 1\n","        # TODO 2: For each class, calcuate P(c)\n","        for label,count in class_counts.items():\n","            self.priors[label] = class_counts[label] / len(labels) #해당 클래스에 속하는 문장의 갯수에서 전체 라벨의 갯수(문장의 갯수) 를 나누면 prior를 구할 수 있다.\n","        #########################################################################        \n","\n","    def _set_likelihoods(self, tokens, labels):\n","        \"\"\"\n","        Set likelihood for each token, P(d|c).\n","        First, count the number of each class for each token.\n","        Then, calculate P(d|c) for a given class and token.\n","        \"\"\"\n","        token_dists = {}\n","        class_counts = defaultdict(int)\n","\n","        for i, label in enumerate(tqdm(labels)):#train하는 도큐먼트 수만큼 만복(문장의 수만큼 반복.)\n","            count = 0\n","\n","            ############################ ANSWER HERE ################################\n","            # TODO: Count the number of each class for each token.\n","            # tokens : tokenized_data['train'] -> 2d array.   [[token,token,...,token] , [token,...,token] , ... [token,...,token]]\n","            # word2index :      word2index     -> dictionary. {token1 : 2 , token2 : 3, ....} frequency per unique token\n","            # token_dists:      token_dists \n","            for token in tokens[i]: #한 문장 안에서 토큰을 하나 뽑는다.\n","                if token in self.word2index: # 그 토큰이 인덱스로 치환 가능한 토큰이면서(?)\n","                    if token not in token_dists: #유일한게 뽑은 값일때. initialize\n","                        token_dists[token] = {0:0,1:0} #initialize\n","                    token_dists[token][label] += 1 # 각 토큰이 등장할때 마다. 그 토큰이 속해있는 문장의 클래스가 0일때 혹은 1일때 각각 갯수를 구한다.\n","            #########################################################################        \n","\n","                count += 1 #토큰의 갯수?\n","            #count+=1 #얘로 하면 문장의 갯수를 센다.\n","            class_counts[label] += count #클래스별로 토큰의 총 개수.\n","\n","        for token, dist in tqdm(token_dists.items()): #토큰 하나씩 꺼낸다.\n","            if token not in self.likelihoods: \n","                self.likelihoods[token] = {\n","                    0: (token_dists[token][0]+ self.k) / (class_counts[0] + len(self.word2index)* self.k), # 클래스별로 특정클래스에 속하는 모든 문장의 토큰의 총개수 분에 해당 클래스인 모든 문장에서의 특정 토큰의 갯수를 구한다.\n","                    1: (token_dists[token][1]+ self.k) / (class_counts[1] + len(self.word2index)* self.k),\n","                }\n","\n","    def train(self, input_tokens, labels):\n","        \"\"\"\n","        (input) input_tokens: list of tokenized train data.\n","                labels: train labels for each sentence/document.\n","        \"\"\"\n","        self._set_priors(labels)\n","        self._set_likelihoods(input_tokens, labels)\n","\n","    def inference(self, input_tokens):\n","        \"\"\"\n","        (input) input_tokens: list_of tokenized test data.\n","        \"\"\"\n","        log_prob_0 = 0.0\n","        log_prob_1 = 0.0\n","\n","        for token in input_tokens: #kkma 토큰화 완료된 1d array에서 토큰을 하나씩 꺼낸다.\n","            if token in self.likelihoods: #우리가 일전에 학습했던 토큰 pool에 속한 토큰이면면\n","                log_prob_0 += math.log(self.likelihoods[token][0]) # 0일확률\n","                log_prob_1 += math.log(self.likelihoods[token][1])# 1일확률 구한다.\n","#문맥은 고려 안하나 보네... 그냥 형태소의 성격만 판단한다.\n","        log_prob_0 += math.log(self.priors[0]) #\n","        log_prob_1 += math.log(self.priors[1])\n","\n","        if log_prob_0 >= log_prob_1:###\n","            return 0\n","        else:\n","            return 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xzjVUyBOQEJk"},"source":["#### 3.2 주어진 학습 데이터에 대해 문장 분류 모델을 학습시킵니다."]},{"cell_type":"code","metadata":{"id":"Wt-iUEVRNsRj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2bde06b9-01af-4670-de92-5ece3c77a494"},"source":["# 문장 분류 모델 선언 및 학습\n","classifier = NaiveBayesClassifier(word2index)\n","classifier.train(tokenized_data['train'], train_labels)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:00<00:00, 107822.72it/s]\n","100%|██████████| 10/10 [00:00<00:00, 64726.91it/s]\n","100%|██████████| 56/56 [00:00<00:00, 360246.97it/s]\n"]}]},{"cell_type":"code","source":["classifier.likelihoods"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QgJEc6_vqKTw","outputId":"fec5a361-1559-4f79-a5bc-69da2a0d250e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'정말': {0: 0.0014367816091954025, 1: 0.01729559748427673},\n"," '재미있': {0: 0.0014367816091954025, 1: 0.01729559748427673},\n"," '습니다': {0: 0.04454022988505748, 1: 0.06446540880503145},\n"," '.': {0: 0.08764367816091954, 1: 0.09591194968553458},\n"," '추천': {0: 0.0014367816091954025, 1: 0.01729559748427673},\n"," '하': {0: 0.04454022988505748, 1: 0.0330188679245283},\n"," 'ㅂ니다': {0: 0.0014367816091954025, 1: 0.04874213836477988},\n"," '기대': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n"," '었': {0: 0.08764367816091954, 1: 0.0330188679245283},\n"," '더': {0: 0.030172413793103453, 1: 0.0015723270440251573},\n"," 'ㄴ': {0: 0.030172413793103453, 1: 0.0015723270440251573},\n"," '것': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n"," '보다': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n"," '별': {0: 0.04454022988505748, 1: 0.0015723270440251573},\n"," '로': {0: 0.04454022988505748, 1: 0.0015723270440251573},\n"," '이': {0: 0.05890804597701149, 1: 0.0330188679245283},\n"," '네요': {0: 0.04454022988505748, 1: 0.0015723270440251573},\n"," '지루': {0: 0.030172413793103453, 1: 0.0015723270440251573},\n"," '어서': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n"," '다시': {0: 0.015804597701149427, 1: 0.01729559748427673},\n"," '보': {0: 0.015804597701149427, 1: 0.01729559748427673},\n"," '고': {0: 0.015804597701149427, 1: 0.0330188679245283},\n"," '싶': {0: 0.015804597701149427, 1: 0.01729559748427673},\n"," '다는': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n"," '생각': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n"," '안': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n"," '들': {0: 0.015804597701149427, 1: 0.01729559748427673},\n"," '완전': {0: 0.0014367816091954025, 1: 0.01729559748427673},\n"," '최고': {0: 0.0014367816091954025, 1: 0.0330188679245283},\n"," '!': {0: 0.0014367816091954025, 1: 0.01729559748427673},\n"," '연기': {0: 0.015804597701149427, 1: 0.04874213836477988},\n"," '도': {0: 0.0014367816091954025, 1: 0.09591194968553458},\n"," '연출': {0: 0.0014367816091954025, 1: 0.04874213836477988},\n"," '다': {0: 0.0014367816091954025, 1: 0.0330188679245283},\n"," '만족': {0: 0.0014367816091954025, 1: 0.01729559748427673},\n"," '스럽': {0: 0.0014367816091954025, 1: 0.01729559748427673},\n"," '가': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n"," '좀': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n"," '좋': {0: 0.015804597701149427, 1: 0.0330188679245283},\n"," '았': {0: 0.0014367816091954025, 1: 0.0330188679245283},\n"," '배우': {0: 0.0014367816091954025, 1: 0.01729559748427673},\n"," '분': {0: 0.0014367816091954025, 1: 0.01729559748427673},\n"," '기념일': {0: 0.0014367816091954025, 1: 0.01729559748427673},\n"," '에': {0: 0.015804597701149427, 1: 0.01729559748427673},\n"," '방문': {0: 0.0014367816091954025, 1: 0.01729559748427673},\n"," '는데': {0: 0.0014367816091954025, 1: 0.01729559748427673},\n"," '전반적': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n"," '으로': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n"," '저': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n"," '는': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n"," 'CG': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n"," '조금': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n"," '신경': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n"," '쓰': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n"," '으면': {0: 0.015804597701149427, 1: 0.0015723270440251573},\n"," '겠': {0: 0.015804597701149427, 1: 0.0015723270440251573}}"]},"metadata":{},"execution_count":49}]},{"cell_type":"markdown","metadata":{"id":"h79XWrsnQJtN"},"source":["### 4. Evaluation"]},{"cell_type":"markdown","metadata":{"id":"Pjk05W136d5o"},"source":["각각의 Test 데이터에 대해 정답값을 추론하고 Accuracy를 구합니다."]},{"cell_type":"code","metadata":{"id":"Fe-fOScGNzH3","colab":{"base_uri":"https://localhost:8080/"},"outputId":"52a72430-0efc-4e4d-89d4-de1366f49b0a"},"source":["# Test 데이터 inference\n","preds = []\n","for test_tokens in tqdm(tokenized_data['test']):\n","    pred = classifier.inference(test_tokens)\n","    preds.append(pred)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:00<00:00, 2704.26it/s]\n"]}]},{"cell_type":"code","metadata":{"id":"hrYMTKM10vYk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bd74018f-2eed-404a-c39a-86f1b7812eee"},"source":["# Accuracy 측정\n","from sklearn.metrics import accuracy_score\n","\n","print(accuracy_score(test_labels, preds))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.0\n"]}]},{"cell_type":"code","metadata":{"id":"4b68-Bh5zArE"},"source":[],"execution_count":null,"outputs":[]}]}